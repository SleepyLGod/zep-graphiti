# ===========================================
# Graphiti + Neo4j + OpenTelemetry Configuration
# ===========================================
# Copy this file to .env and fill in your values

# ----- LLM Configuration -----
# Priority: Local LLM > Gemini > OpenAI > DeepSeek (first available is used)

# Option 1: Local LLM via Ollama or vLLM (RECOMMENDED)
# No API costs, runs entirely locally
# Ollama: curl -fsSL https://ollama.com/install.sh | sh && ollama pull qwen2.5:7b
# vLLM: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct --port 8000
LOCAL_LLM_ENABLED=true
LOCAL_LLM_BASE_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=qwen2.5:7b
# For vLLM (faster, better GPU utilization):
# LOCAL_LLM_BASE_URL=http://localhost:8000/v1
# LOCAL_LLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# Option 2: OpenAI
# OPENAI_API_KEY=sk-your-openai-key-here

# Option 3: Google Gemini (supports structured output, but region restricted)
# GEMINI_API_KEY=your-gemini-api-key-here
# GEMINI_MODEL=gemini-2.0-flash

# Option 4: DeepSeek (OpenAI-compatible)
# WARNING: DeepSeek does NOT support response_format parameter!
# DEEPSEEK_API_KEY=sk-your-deepseek-key-here
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
# DEEPSEEK_MODEL=deepseek-chat

# ----- Embedding Configuration -----
# Set to "local" to use sentence-transformers (no API key needed)
# Set to "openai" to use OpenAI embeddings (requires OPENAI_API_KEY)
# Set to "gemini" to use Gemini embeddings (requires GEMINI_API_KEY)
# Set to "ollama" to use Ollama embeddings
EMBEDDING_PROVIDER=local

# Local embedding model (only used when EMBEDDING_PROVIDER=local)
# Options: 'all-MiniLM-L6-v2' (384 dims, fast), 'all-mpnet-base-v2' (768 dims, better quality)
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Ollama embedding (only used when EMBEDDING_PROVIDER=ollama)
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_EMBEDDING_DIM=768

# ----- Neo4j Configuration -----
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

