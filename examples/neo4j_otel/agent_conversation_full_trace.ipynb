{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zep Agent Conversation with Full Trace\n",
    "\n",
    "**Goal**: See every function call, LLM prompt/response, and Neo4j query from user input to database.\n",
    "\n",
    "**Approach**: Use `graphiti_core` directly to simulate Zep Cloud API behavior, with comprehensive tracing.\n",
    "\n",
    "The markdown description is generated by Claude and edited by me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Arch Overview\n",
    "\n",
    "### Zep Cloud Production Architecture\n",
    "\n",
    "In production, Zep Cloud has a multi-layer architecture:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   Agent Application                                       \u2502\n",
    "\u2502  (Uses Zep Python SDK: zep_cloud.client)                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                      \u2502 HTTPS REST API\n",
    "                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Zep Server (Go)                                          \u2502\n",
    "\u2502  - Manages users, sessions, threads                       \u2502\n",
    "\u2502  - Handles authentication and rate limiting               \u2502\n",
    "\u2502  - Orchestrates memory operations                         \u2502\n",
    "\u2502  Source: zep/legacy/src/api/apihandlers/                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                      \u2502 HTTP REST API\n",
    "                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Graphiti Server (Python FastAPI)                         \u2502\n",
    "\u2502  - Wraps graphiti_core library                            \u2502\n",
    "\u2502  - Provides REST endpoints for graph operations           \u2502\n",
    "\u2502  - Handles async processing queue                         \u2502\n",
    "\u2502  Source: zep-graphiti/server/graph_service/               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                      \u2502 Python function calls\n",
    "                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  graphiti_core (Python Library)                           \u2502\n",
    "\u2502  - Temporal knowledge graph engine                        \u2502\n",
    "\u2502  - Entity extraction and resolution                       \u2502\n",
    "\u2502  - Bi-temporal data model                                 \u2502\n",
    "\u2502  Source: zep-graphiti/graphiti_core/                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                     \u2502                                     \u2502\n",
    "\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n",
    "\u2502    \u25bc                                 \u25bc                    \u2502\n",
    "\u2502  LLM API                        Neo4j Database            \u2502\n",
    "\u2502  (Entity extraction,            (Graph storage,           \u2502\n",
    "\u2502   summarization,                 Cypher queries)          \u2502\n",
    "\u2502   reranking)                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "like a **Layered Microservices** pattern:\n",
    "\n",
    "| Layer | Component | Role | Responsibility |\n",
    "|-------|-----------|------|----------------|\n",
    "| **Client / Interface** | Zep Python SDK | HTTP Client Wrapper | Converts function calls (`memory.add()`) to REST API requests, handles retries and authentication. **Contains no business logic.** |\n",
    "| **Gateway / Orchestration** | Zep Server (Go) | API Gateway & User Management | Multi-tenant management (AuthN/AuthZ), rate limiting, request routing. Dispatches tasks to downstream services. |\n",
    "| **Worker / Execution** | Graphiti Server (FastAPI) | Async Task Runner | Maintains async job queue. Since LLM processing is slow (seconds to tens of seconds), it converts memory operations into async jobs to prevent blocking the gateway. |\n",
    "| **Business Logic / \\\"The Brain\\\"** | Graphiti Core (Library) | Core Logic Engine | Contains all prompt templates (`prompts/*.py`), LLM interaction flow control (extract \u2192 dedupe \u2192 validate), and dynamic Cypher query generation (`*_db_queries.py`). **Stateless logic code.** |\n",
    "\n",
    "**Evidence from Source Code:**\n",
    "\n",
    "1. **Zep Python SDK** - HTTP client with no logic:\n",
    "   ```python\n",
    "   # zep/examples/python/graph_example/graph_example.py\n",
    "   client = AsyncZep(api_key=API_KEY)  # Just an HTTP client\n",
    "   await client.graph.add(graph_id=graph_id, data=\"...\", type=\"text\")\n",
    "   ```\n",
    "\n",
    "2. **Zep Server (Go)** - Routes to Graphiti:\n",
    "   ```go\n",
    "   // zep/legacy/src/store/memory_ce.go\n",
    "   graphiti.I().PutMemory(ctx, session.SessionID, memoryMessages.Messages, true)\n",
    "   graphiti.I().GetMemory(ctx, graphiti.GetMemoryRequest{...})\n",
    "   ```\n",
    "\n",
    "3. **Graphiti Server** - Async queue for slow LLM operations:\n",
    "   ```python\n",
    "   # zep-graphiti/server/graph_service/routers/ingest.py\n",
    "   class AsyncWorker:\n",
    "       def __init__(self):\n",
    "           self.queue = asyncio.Queue()  # Async job queue\n",
    "   \n",
    "   @router.post('/messages', status_code=status.HTTP_202_ACCEPTED)  # Returns immediately\n",
    "   async def add_messages(...):\n",
    "       await async_worker.queue.put(partial(add_messages_task, m))  # Queue the job\n",
    "   ```\n",
    "\n",
    "4. **Graphiti Core** - All business logic:\n",
    "   - `prompts/extract_nodes.py` - Entity extraction prompts\n",
    "   - `prompts/dedupe_nodes.py` - Entity deduplication prompts\n",
    "   - `prompts/extract_edges.py` - Relationship extraction prompts\n",
    "   - `models/nodes/node_db_queries.py` - Dynamic Cypher generation (Neo4j/FalkorDB/Kuzu)\n",
    "   - `graphiti.py:add_episode()` - Orchestrates the full extraction pipeline\n",
    "\n",
    "### My Setup\n",
    "\n",
    "I just bypass Zep Server and Graphiti Server, calling `graphiti_core` directly.\n",
    "\n",
    "This can give **complete visibility** into every operation easily\n",
    "\n",
    "later I may run with Zep Server and Graphiti Server, and compare the results.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  - ZepSimulator: Mimics Zep SDK patterns                   \u2502\n",
    "\u2502  - TraceLogger: Captures all operations                    \u2502\n",
    "\u2502  - Agent: Simple conversation loop                         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                      \u2502 Direct Python calls (OTEL tracing)\n",
    "                      \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  graphiti_core (Python Library)                            \u2502\n",
    "\u2502  - Full source code access                                 \u2502\n",
    "\u2502  - All internal operations visible                         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n",
    "\u2502    \u25bc                                 \u25bc                     \u2502\n",
    "\u2502  vLLM Server                    Local Neo4j                \u2502\n",
    "\u2502  (Qwen2.5-32B on H100)          (bolt://localhost:7687)    \u2502\n",
    "\u2502  - HTTP traffic logged          - All queries logged       \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zep Cloud API \u2192 Graphiti Complete Mapping\n",
    "\n",
    "This mapping is verified from source code analysis.\n",
    "\n",
    "### API Mapping Table\n",
    "\n",
    "| Zep Python SDK | Zep Server (Go) | Graphiti Server | graphiti_core | Purpose |\n",
    "|----------------|-----------------|-----------------|---------------|----------|\n",
    "| `thread.add_messages()` | `memory_ce.go:_initializeProcessingMemory()` | `POST /messages` | `add_episode()` | Store conversation messages |\n",
    "| `thread.get_user_context()` | `memory_ce.go:_get()` | `POST /get-memory` | `search()` | Retrieve relevant context |\n",
    "| `session.get_memory()` | `memory_ce.go:_get()` | `POST /get-memory` | `search()` | Get session memory |\n",
    "| `graph.search()` | `memory_ce.go:_searchSessions()` | `POST /search` | `search()` | Search knowledge graph |\n",
    "| `user.add()` | `userstore_ce.go:_processCreatedUser()` | `POST /entity-node` | `save_entity_node()` | Create user entity |\n",
    "| `graph.add()` | N/A | `POST /entity-node` | `save_entity_node()` | Add structured data |\n",
    "| `user.delete()` | `user_handlers.go:DeleteUserHandler()` | `DELETE /group/{group_id}` | `delete_group()` | Delete user and all data |\n",
    "| `fact.get()` | `fact_handlers_ce.go:getFact()` | `GET /entity-edge/{uuid}` | `EntityEdge.get_by_uuid()` | Get a specific fact |\n",
    "| `fact.delete()` | `fact_handlers_ce.go:deleteSessionFact()` | `DELETE /entity-edge/{uuid}` | `delete_entity_edge()` | Delete a fact |\n",
    "| `session.delete_memory()` | `memory_handlers_ce.go:deleteMemory()` | `DELETE /group/{group_id}` | `delete_group()` | Delete session memory |\n",
    "| `episode.delete()` | N/A | `DELETE /episode/{uuid}` | `delete_episodic_node()` | Delete an episode |\n",
    "\n",
    "### Source Code Evidence\n",
    "\n",
    "> **Just FYI, no need to read this.**\n",
    "\n",
    "**1. thread.add_messages() \u2192 add_episode()**\n",
    "\n",
    "From `zep/legacy/src/store/memory_ce.go`:\n",
    "```go\n",
    "func (dao *memoryDAO) _initializeProcessingMemory(...) error {\n",
    "    err := graphiti.I().PutMemory(ctx, session.SessionID, memoryMessages.Messages, true)\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/ingest.py`:\n",
    "```python\n",
    "@router.post('/messages', status_code=status.HTTP_202_ACCEPTED)\n",
    "async def add_messages(request: AddMessagesRequest, graphiti: ZepGraphitiDep):\n",
    "    async def add_messages_task(m: Message):\n",
    "        await graphiti.add_episode(\n",
    "            uuid=m.uuid,\n",
    "            group_id=request.group_id,\n",
    "            name=m.name,\n",
    "            episode_body=f'{m.role or \"\"}({m.role_type}): {m.content}',\n",
    "            ...\n",
    "        )\n",
    "```\n",
    "\n",
    "**2. thread.get_user_context() \u2192 search()**\n",
    "\n",
    "From `zep/legacy/src/store/memory_ce.go`:\n",
    "```go\n",
    "func (dao *memoryDAO) _get(...) (*models.Memory, error) {\n",
    "    memory, err := graphiti.I().GetMemory(ctx, graphiti.GetMemoryRequest{\n",
    "        GroupID:  groupID,\n",
    "        MaxFacts: 5,\n",
    "        Messages: mForRetrieval,\n",
    "    })\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/retrieve.py`:\n",
    "```python\n",
    "@router.post('/get-memory', status_code=status.HTTP_200_OK)\n",
    "async def get_memory(request: GetMemoryRequest, graphiti: ZepGraphitiDep):\n",
    "    combined_query = compose_query_from_messages(request.messages)\n",
    "    result = await graphiti.search(\n",
    "        group_ids=[request.group_id],\n",
    "        query=combined_query,\n",
    "        num_results=request.max_facts,\n",
    "    )\n",
    "```\n",
    "\n",
    "**3. user.delete() \u2192 delete_group()**\n",
    "\n",
    "From `zep/legacy/src/api/apihandlers/user_handlers.go`:\n",
    "```go\n",
    "func DeleteUserHandler(appState *models.AppState) http.HandlerFunc {\n",
    "    return func(w http.ResponseWriter, r *http.Request) {\n",
    "        err := userStore.DeleteUser(ctx, userID)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/zep_graphiti.py`:\n",
    "```python\n",
    "async def delete_group(self, group_id: str):\n",
    "    edges = await EntityEdge.get_by_group_ids(self.driver, [group_id])\n",
    "    for edge in edges:\n",
    "        await edge.delete(self.driver)\n",
    "    # Also deletes nodes and episodes\n",
    "```\n",
    "\n",
    "**4. fact.get() / fact.delete() \u2192 get_entity_edge() / delete_entity_edge()**\n",
    "\n",
    "From `zep/legacy/src/api/apihandlers/fact_handlers_ce.go`:\n",
    "```go\n",
    "func getFact(...) (*models.Fact, error) { ... }\n",
    "func deleteSessionFact(...) error { ... }\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/ingest.py`:\n",
    "```python\n",
    "@router.delete('/entity-edge/{uuid}')\n",
    "async def delete_entity_edge(uuid: str, graphiti: ZepGraphitiDep):\n",
    "    await graphiti.delete_entity_edge(uuid)\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/retrieve.py`:\n",
    "```python\n",
    "@router.get('/entity-edge/{uuid}')\n",
    "async def get_entity_edge(uuid: str, graphiti: ZepGraphitiDep):\n",
    "    return await graphiti.get_entity_edge(uuid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Internal Flow: What Happens Inside Each Operation\n",
    "\n",
    "### 3.1 add_episode() Internal Flow\n",
    "\n",
    "When you call `graphiti.add_episode()`, here's what happens internally.\n",
    "\n",
    "```\n",
    "add_episode(episode_body=\"Alice Chen(user): Hi, I'm Alice Chen. I work at TechCorp...\")\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 0: Query Previous Episodes\n",
    "\u2502   Neo4j: MATCH (e:Episodic) WHERE e.valid_at <= $reference_time AND e.group_id IN $group_ids\n",
    "\u2502          RETURN e ORDER BY e.valid_at DESC LIMIT $num_episodes\n",
    "\u2502   Purpose: Get conversation context for entity extraction\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 1: Extract Entities (LLM Call - extract_nodes.extract_message)\n",
    "\u2502   System: \"You are an AI assistant that extracts entity nodes from conversational messages...\"\n",
    "\u2502   User: \"<ENTITY TYPES>...</ENTITY TYPES> <CURRENT MESSAGE>Alice Chen(user): Hi, I'm Alice Chen...</CURRENT MESSAGE>\"\n",
    "\u2502   Response: {\"extracted_entities\": [{\"name\": \"Alice Chen\", \"entity_type_id\": 0}, {\"name\": \"TechCorp\", \"entity_type_id\": 0}]}\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 2: Search for Existing Entities (Neo4j - Parallel Queries)\n",
    "\u2502   For EACH extracted entity, run two parallel searches:\n",
    "\u2502   \u2502\n",
    "\u2502   \u251c\u2500\u25ba 2a: BM25 Fulltext Search\n",
    "\u2502   \u2502   Neo4j: CALL db.index.fulltext.queryNodes(\\\"node_name_and_summary\\\", $query, {limit: $limit})\n",
    "\u2502   \u2502          YIELD node AS n, score WHERE n.group_id IN $group_ids\n",
    "\u2502   \u2502\n",
    "\u2502   \u2514\u2500\u25ba 2b: Cosine Similarity Search\n",
    "\u2502       Neo4j: MATCH (n:Entity) WHERE n.group_id IN $group_ids\n",
    "\u2502              WITH n, vector.similarity.cosine(n.name_embedding, $search_vector) AS score\n",
    "\u2502              WHERE score > $min_score RETURN n ORDER BY score DESC\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 3: Deduplicate Entities (LLM Call - dedupe_nodes.nodes)\n",
    "\u2502   System: \"You are a helpful assistant that determines whether or not ENTITIES extracted from a conversation are duplicates...\"\n",
    "\u2502   User: \"<ENTITIES>[extracted entities]</ENTITIES> <EXISTING ENTITIES>[candidates from Neo4j]</EXISTING ENTITIES>\"\n",
    "\u2502   Response: {\"entity_resolutions\": [{\"id\": 0, \"name\": \"Alice Chen\", \"duplicate_idx\": -1, \"duplicates\": []}, ...]}\n",
    "\u2502   Purpose: Match new entities to existing ones or confirm they are new\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 4: Extract Relationships (LLM Call - extract_edges.edge)\n",
    "\u2502   System: \"You are an expert fact extractor that extracts fact triples from text...\"\n",
    "\u2502   User: \"<ENTITIES>[resolved entities]</ENTITIES> <CURRENT_MESSAGE>...</CURRENT_MESSAGE> <REFERENCE_TIME>...</REFERENCE_TIME>\"\n",
    "\u2502   Response: {\"edges\": [{\"relation_type\": \"WORKS_AT\", \"source_entity_id\": 0, \"target_entity_id\": 1,\n",
    "\u2502              \"fact\": \"Alice Chen works at TechCorp as a senior software engineer.\", \"valid_at\": \"2026-01-31T13:35:08Z\"}]}\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 5: Search for Existing Edges (Neo4j - Multiple Queries)\n",
    "\u2502   \u2502\n",
    "\u2502   \u251c\u2500\u25ba 5a: Direct Edge Lookup\n",
    "\u2502   \u2502   Neo4j: MATCH (n:Entity {uuid: $source_node_uuid})-[e:RELATES_TO]->(m:Entity {uuid: $target_node_uuid}) RETURN e\n",
    "\u2502   \u2502\n",
    "\u2502   \u251c\u2500\u25ba 5b: BM25 Fulltext Search on Edges\n",
    "\u2502   \u2502   Neo4j: CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", $query, {limit: $limit})\n",
    "\u2502   \u2502          YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_TO {uuid: rel.uuid}]->(m:Entity)\n",
    "\u2502   \u2502\n",
    "\u2502   \u2514\u2500\u25ba 5c: Cosine Similarity Search on Edges\n",
    "\u2502       Neo4j: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids\n",
    "\u2502              WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_vector) AS score\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 6: Deduplicate Edges (LLM Call - dedupe_edges.resolve_edge) [Only if existing edges found]\n",
    "\u2502   System: \"You are a helpful assistant that de-duplicates facts from fact lists...\"\n",
    "\u2502   User: \"<EXISTING FACTS>[...]</EXISTING FACTS> <FACT INVALIDATION CANDIDATES>[...]</FACT INVALIDATION CANDIDATES> <NEW FACT>...</NEW FACT>\"\n",
    "\u2502   Response: {\"duplicate_facts\": [], \"contradicted_facts\": [], \"fact_type\": \"DEFAULT\"}\n",
    "\u2502   Purpose: Detect duplicate or contradicting facts\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 7: Generate Entity Summaries (LLM Calls - PARALLEL - extract_nodes.extract_summary)\n",
    "\u2502   For EACH new or updated entity, run in parallel:\n",
    "\u2502   System: \"You are a helpful assistant that extracts entity summaries from the provided text...\"\n",
    "\u2502   User: \"<MESSAGES>[conversation history]</MESSAGES> <ENTITY>{name, summary, entity_types, attributes}</ENTITY>\"\n",
    "\u2502   Response: {\"summary\": \"Alice Chen works at TechCorp as a senior software engineer.\"}\n",
    "\u2502   parallel execution\n",
    "\u2502\n",
    "\u2514\u2500\u25ba Step 8: Write to Neo4j (Implicit - happens during steps above)\n",
    "    Entities and edges are created/updated as they are processed.\n",
    "    The EpisodicNode is also created to store the original message.\n",
    "```\n",
    "\n",
    "### 3.2 search() Internal Flow\n",
    "\n",
    "When you call `graphiti.search()`, the system uses **two search methods in parallel** for edge retrieval.\n",
    "\n",
    "```\n",
    "search(query=\"What does Alice work on?\", group_ids=[\"demo_session_...\"], num_results=10)\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 1: Generate Query Embedding (Local Embedder)\n",
    "\u2502   Embedder: sentence-transformers/all-MiniLM-L6-v2\n",
    "\u2502   encode(\"What does Alice work on?\") \u2192 [0.12, -0.34, ...] (384 dimensions)\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 2: Execute Search Methods (Parallel)\n",
    "\u2502   \u2502\n",
    "\u2502   \u251c\u2500\u25ba 2a: BM25 Fulltext Search on Relationships\n",
    "\u2502   \u2502   Neo4j: CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", $query, {limit: $limit})\n",
    "\u2502   \u2502          YIELD relationship AS rel, score\n",
    "\u2502   \u2502          MATCH (n:Entity)-[e:RELATES_TO {uuid: rel.uuid}]->(m:Entity)\n",
    "\u2502   \u2502          WHERE e.group_id IN $group_ids\n",
    "\u2502   \u2502          RETURN e, n, m ORDER BY score DESC\n",
    "\u2502   \u2502\n",
    "\u2502   \u2514\u2500\u25ba 2b: Cosine Similarity Search on Relationships\n",
    "\u2502       Neo4j: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity)\n",
    "\u2502              WHERE e.group_id IN $group_ids\n",
    "\u2502              WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_vector) AS score\n",
    "\u2502              WHERE score > $min_score\n",
    "\u2502              RETURN e, n, m ORDER BY score DESC LIMIT $limit\n",
    "\u2502\n",
    "\u251c\u2500\u25ba Step 3: Combine and Deduplicate Results\n",
    "\u2502   Merge results from both search methods\n",
    "\u2502   Remove duplicates based on edge UUID\n",
    "\u2502   Sort by relevance score\n",
    "\u2502\n",
    "\u2514\u2500\u25ba Step 4: Return Top Results\n",
    "    Return: [EntityEdge(fact=\"Alice Chen is currently leading Project Phoenix.\", ...),\n",
    "             EntityEdge(fact=\"Project Phoenix has a deadline on February 15th.\", ...),\n",
    "             EntityEdge(fact=\"Alice Chen works at TechCorp as a senior software engineer.\", ...)]\n",
    "```\n",
    "\n",
    "**Search Methods by Data Type** (from `graphiti_core/search/search.py`):\n",
    "- **EntityEdge**: BM25 Fulltext + Cosine Similarity (+ optional BFS, LLM rerank)\n",
    "- **EntityNode**: BM25 Fulltext + Cosine Similarity (+ optional BFS)\n",
    "- **EpisodicNode**: BM25 Fulltext only\n",
    "- **Community**: BM25 Fulltext + Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Setup\n",
    "\n",
    "The following cells configure the same environment as `graphiti_neo4j_otel_demo.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Jan 14 2026, 19:35:58) [Clang 21.1.4 ]\n",
      "Working dir: /mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.1: Imports and Basic Logging Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timezone\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j URI: bolt://localhost:7687\n",
      "Local LLM Enabled: True\n",
      "  Base URL: http://localhost:8801/v1\n",
      "  Model: Qwen/Qwen2.5-32B-Instruct\n",
      "Embedding Provider: local\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.2: Load Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j Configuration\n",
    "neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# Local LLM Configuration\n",
    "local_llm_enabled = os.environ.get('LOCAL_LLM_ENABLED', 'false').lower() == 'true'\n",
    "local_llm_base_url = os.environ.get('LOCAL_LLM_BASE_URL', 'http://localhost:8000/v1')\n",
    "local_llm_model = os.environ.get('LOCAL_LLM_MODEL', 'Qwen/Qwen2.5-32B-Instruct')\n",
    "local_llm_api_key = os.environ.get('LOCAL_LLM_API_KEY', 'vllm')\n",
    "\n",
    "# Embedding Configuration\n",
    "embedding_provider = os.environ.get('EMBEDDING_PROVIDER', 'local')\n",
    "local_embedding_model = os.environ.get('LOCAL_EMBEDDING_MODEL', 'all-MiniLM-L6-v2')\n",
    "\n",
    "print(f'Neo4j URI: {neo4j_uri}')\n",
    "print(f'Local LLM Enabled: {local_llm_enabled}')\n",
    "if local_llm_enabled:\n",
    "    print(f'  Base URL: {local_llm_base_url}')\n",
    "    print(f'  Model: {local_llm_model}')\n",
    "print(f'Embedding Provider: {embedding_provider}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry tracing configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.3: Configure OpenTelemetry Tracing\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "\n",
    "def setup_otel_tracing():\n",
    "    \"\"\"Configure OpenTelemetry to output traces to console\"\"\"\n",
    "    resource = Resource(attributes={\n",
    "        'service.name': 'zep-agent-full-trace',\n",
    "        'service.version': '1.0.0',\n",
    "    })\n",
    "    \n",
    "    provider = TracerProvider(resource=resource)\n",
    "    provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "    trace.set_tracer_provider(provider)\n",
    "    \n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "otel_tracer = setup_otel_tracing()\n",
    "print(\"OpenTelemetry tracing configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformerEmbedder class defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.4: Define Local Embedder (sentence-transformers)\n",
    "from graphiti_core.embedder.client import EmbedderClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceTransformerEmbedder(EmbedderClient):\n",
    "    \"\"\"\n",
    "    Local embedder using sentence-transformers.\n",
    "    No API key required - runs entirely locally.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        print(f\"Loading sentence-transformers model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Model loaded. Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    async def create(\n",
    "        self, input_data: str | list[str] | Iterable[int] | Iterable[Iterable[int]]\n",
    "    ) -> list[float]:\n",
    "        \"\"\"Create embedding for input text.\"\"\"\n",
    "        if isinstance(input_data, str):\n",
    "            text = input_data\n",
    "        elif isinstance(input_data, list) and len(input_data) > 0:\n",
    "            text = input_data[0] if isinstance(input_data[0], str) else str(input_data[0])\n",
    "        else:\n",
    "            text = str(input_data)\n",
    "        \n",
    "        loop = asyncio.get_running_loop()\n",
    "        embedding = await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "        )\n",
    "        return embedding\n",
    "    \n",
    "    async def create_batch(self, input_data_list: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Create embeddings for a batch of texts.\"\"\"\n",
    "        loop = asyncio.get_running_loop()\n",
    "        embeddings = await loop.run_in_executor(\n",
    "            None,\n",
    "            lambda: self.model.encode(input_data_list, convert_to_numpy=True).tolist()\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "print(\"SentenceTransformerEmbedder class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLMRerankerClient class defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.5: vLLM Reranker Client\n",
    "#\n",
    "# Custom CrossEncoderClient that uses vLLM for reranking.\n",
    "# Uses direct scoring (0-100) instead of logprobs for better compatibility.\n",
    "\n",
    "import re\n",
    "from graphiti_core.cross_encoder.client import CrossEncoderClient\n",
    "from graphiti_core.helpers import semaphore_gather\n",
    "\n",
    "class VLLMRerankerClient(CrossEncoderClient):\n",
    "    \"\"\"CrossEncoderClient implementation using vLLM with direct relevance scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, model: str):\n",
    "        self.client = client  # AsyncOpenAI client\n",
    "        self.model = model\n",
    "    \n",
    "    async def rank(self, query: str, passages: list[str]) -> list[tuple[str, float]]:\n",
    "        if not passages:\n",
    "            return []\n",
    "        if len(passages) == 1:\n",
    "            return [(passages[0], 1.0)]\n",
    "        \n",
    "        async def score_passage(passage: str) -> tuple[str, float]:\n",
    "            prompt = f\"\"\"Rate relevance 0-100. Query: {query} Passage: {passage} Number only:\"\"\"\n",
    "            try:\n",
    "                response = await self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=5,\n",
    "                )\n",
    "                score_text = response.choices[0].message.content.strip()\n",
    "                match = re.search(r'\\\\d+', score_text)\n",
    "                score = min(100, max(0, int(match.group()))) / 100.0 if match else 0.5\n",
    "                return (passage, score)\n",
    "            except Exception as e:\n",
    "                print(f'Reranker error: {e}')\n",
    "                return (passage, 0.0)\n",
    "        \n",
    "        results = await semaphore_gather(*[score_passage(p) for p in passages])\n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('VLLMRerankerClient class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing local embedder with model: all-MiniLM-L6-v2\n",
      "Loading sentence-transformers model: all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Using Local LLM at http://localhost:8801/v1\n",
      "Model: Qwen/Qwen2.5-32B-Instruct\n",
      "Timeout: 600s\n",
      "Graphiti initialized with VLLMRerankerClient, connected to bolt://localhost:7687\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.6: Initialize Graphiti with vLLM Reranker\n",
    "#\n",
    "# IMPORTANT: Make sure vLLM is running on port 8801:\n",
    "# CUDA_VISIBLE_DEVICES=4,5 uv run vllm serve Qwen/Qwen2.5-32B-Instruct \\\n",
    "#     --port 8801 --api-key vllm --tensor-parallel-size 2 \\\n",
    "#     --max-model-len 16384 --enforce-eager --gpu-memory-utilization 0.85\n",
    "\n",
    "from graphiti_core import Graphiti\n",
    "from graphiti_core.llm_client.config import LLMConfig\n",
    "from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Timeout Configuration\n",
    "LLM_TIMEOUT_SECONDS = 600  # 10 minutes per LLM call\n",
    "\n",
    "# Initialize Local Embedder\n",
    "print(f'Initializing local embedder with model: {local_embedding_model}')\n",
    "embedder = SentenceTransformerEmbedder(model_name=local_embedding_model)\n",
    "\n",
    "# Initialize LLM Client\n",
    "if not local_llm_enabled:\n",
    "    raise ValueError('LOCAL_LLM_ENABLED must be true in .env')\n",
    "\n",
    "print(f'Using Local LLM at {local_llm_base_url}')\n",
    "print(f'Model: {local_llm_model}')\n",
    "print(f'Timeout: {LLM_TIMEOUT_SECONDS}s')\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    api_key=local_llm_api_key,\n",
    "    model=local_llm_model,\n",
    "    small_model=local_llm_model,\n",
    "    base_url=local_llm_base_url,\n",
    ")\n",
    "\n",
    "custom_openai_client = AsyncOpenAI(\n",
    "    api_key=local_llm_api_key,\n",
    "    base_url=local_llm_base_url,\n",
    "    timeout=LLM_TIMEOUT_SECONDS,\n",
    ")\n",
    "\n",
    "llm_client = OpenAIGenericClient(\n",
    "    config=llm_config,\n",
    "    client=custom_openai_client,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# Use VLLMRerankerClient with the same AsyncOpenAI client\n",
    "cross_encoder = VLLMRerankerClient(client=custom_openai_client, model=local_llm_model)\n",
    "\n",
    "# Initialize Graphiti\n",
    "graphiti = Graphiti(\n",
    "    uri=neo4j_uri,\n",
    "    user=neo4j_user,\n",
    "    password=neo4j_password,\n",
    "    llm_client=llm_client,\n",
    "    embedder=embedder,\n",
    "    cross_encoder=cross_encoder,\n",
    "    tracer=otel_tracer,\n",
    "    trace_span_prefix='zep.graphiti',\n",
    ")\n",
    "\n",
    "print(f'Graphiti initialized with VLLMRerankerClient, connected to {neo4j_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices and constraints built successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.7: Build Indices and Constraints\n",
    "await graphiti.build_indices_and_constraints()\n",
    "print(\"Indices and constraints built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trace System Implementation\n",
    "\n",
    "We implement a dual-output trace system:\n",
    "- **Raw JSON** \u2192 `trace_raw.jsonl` (for debugging)\n",
    "- **Pretty Print** \u2192 stdout (for reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Enhanced TraceLogger - File-Only Output System\n",
    "#\n",
    "# This enhanced TraceLogger outputs ALL logs to files only (no console output).\n",
    "# It captures: API calls, LLM calls, Neo4j queries (READ + WRITE), Embeddings, Parallel ops, OTEL spans.\n",
    "#\n",
    "# Log Files:\n",
    "#   - trace_main.jsonl      : Main trace log (API, Graphiti calls, sections)\n",
    "#   - trace_llm.jsonl       : LLM requests and responses\n",
    "#   - trace_neo4j.jsonl     : Neo4j READ queries\n",
    "#   - trace_neo4j_write.jsonl: Neo4j WRITE operations (CREATE, MERGE, SET)\n",
    "#   - trace_embeddings.jsonl: Embedding generation calls\n",
    "#   - trace_parallel.jsonl  : Parallel operation tracking\n",
    "#   - trace_otel.jsonl      : OpenTelemetry spans\n",
    "\n",
    "class EnhancedTraceLogger:\n",
    "    \"\"\"\n",
    "    Enhanced trace logger that outputs to separate files with NO console output.\n",
    "    Captures complete information about all operations including embeddings and writes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = '.'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open all log files\n",
    "        self.files = {\n",
    "            'main': open(self.output_dir / 'trace_main.jsonl', 'w'),\n",
    "            'llm': open(self.output_dir / 'trace_llm.jsonl', 'w'),\n",
    "            'neo4j': open(self.output_dir / 'trace_neo4j.jsonl', 'w'),\n",
    "            'neo4j_write': open(self.output_dir / 'trace_neo4j_write.jsonl', 'w'),\n",
    "            'embeddings': open(self.output_dir / 'trace_embeddings.jsonl', 'w'),\n",
    "            'parallel': open(self.output_dir / 'trace_parallel.jsonl', 'w'),\n",
    "            'otel': open(self.output_dir / 'trace_otel.jsonl', 'w'),\n",
    "        }\n",
    "        \n",
    "        self.indent_level = 0\n",
    "        self.counters = {\n",
    "            'llm_call': 0,\n",
    "            'neo4j_query': 0,\n",
    "            'neo4j_write': 0,\n",
    "            'embedding': 0,\n",
    "            'parallel_task': 0,\n",
    "            'otel_span': 0,\n",
    "        }\n",
    "        \n",
    "        # Track parallel operations\n",
    "        self.active_parallel_tasks = {}\n",
    "        \n",
    "        # Log initialization (only to file)\n",
    "        self._write('main', {\n",
    "            'level': 'INIT',\n",
    "            'message': 'EnhancedTraceLogger initialized',\n",
    "            'log_files': list(self.files.keys()),\n",
    "        })\n",
    "    \n",
    "    def _write(self, file_key: str, entry: dict):\n",
    "        \"\"\"Write JSON entry to specified log file\"\"\"\n",
    "        entry['timestamp'] = datetime.now(timezone.utc).isoformat()\n",
    "        self.files[file_key].write(json.dumps(entry, default=str) + '\\n')\n",
    "        self.files[file_key].flush()\n",
    "    \n",
    "    def _indent(self) -> str:\n",
    "        return '   ' * self.indent_level\n",
    "    \n",
    "    # ========== Main Trace Events ==========\n",
    "    \n",
    "    def log_main(self, event: str, data: dict = None):\n",
    "        \"\"\"Generic main log entry (for any high-level event)\"\"\"\n",
    "        self._write('main', {'level': 'MAIN', 'event': event, 'data': data})\n",
    "    \n",
    "    def log_section(self, title: str):\n",
    "        \"\"\"Log a section header (NO console output)\"\"\"\n",
    "        self._write('main', {'level': 'SECTION', 'title': title})\n",
    "    \n",
    "    def log_api_call(self, method: str, params: dict):\n",
    "        \"\"\"Log a Zep-equivalent API call\"\"\"\n",
    "        self._write('main', {'level': 'API', 'method': method, 'params': params})\n",
    "    \n",
    "    def log_graphiti_call(self, method: str, params: dict = None):\n",
    "        \"\"\"Log a graphiti_core function call\"\"\"\n",
    "        self._write('main', {'level': 'GRAPHITI_START', 'method': method, 'params': params})\n",
    "        self.indent_level += 1\n",
    "    \n",
    "    def log_graphiti_step(self, step_num: int, description: str):\n",
    "        \"\"\"Log a step within a graphiti operation\"\"\"\n",
    "        self._write('main', {'level': 'GRAPHITI_STEP', 'step': step_num, 'description': description})\n",
    "    \n",
    "    def log_graphiti_end(self, duration_ms: float = None, result_summary: str = None):\n",
    "        \"\"\"End a graphiti operation\"\"\"\n",
    "        self.indent_level = max(0, self.indent_level - 1)\n",
    "        self._write('main', {'level': 'GRAPHITI_END', 'duration_ms': duration_ms, 'result': result_summary})\n",
    "    \n",
    "    def log_result(self, description: str, data: any = None):\n",
    "        \"\"\"Log a result\"\"\"\n",
    "        self._write('main', {'level': 'RESULT', 'description': description, 'data': str(data) if data else None})\n",
    "    \n",
    "    # ========== LLM Events ==========\n",
    "    \n",
    "    def log_llm_request(self, call_number: int, model: str, messages: list, **kwargs):\n",
    "        \"\"\"Log LLM request\"\"\"\n",
    "        self.counters['llm_call'] = call_number\n",
    "        self._write('llm', {\n",
    "            'level': 'LLM_REQUEST',\n",
    "            'call_number': call_number,\n",
    "            'model': model,\n",
    "            'messages': messages,\n",
    "            'params': kwargs,\n",
    "        })\n",
    "    \n",
    "    def log_llm_response(self, call_number: int, content: str, usage: dict, finish_reason: str = None):\n",
    "        \"\"\"Log LLM response\"\"\"\n",
    "        self._write('llm', {\n",
    "            'level': 'LLM_RESPONSE',\n",
    "            'call_number': call_number,\n",
    "            'content': content,\n",
    "            'usage': usage,\n",
    "            'finish_reason': finish_reason,\n",
    "        })\n",
    "    \n",
    "    # ========== Neo4j Events ==========\n",
    "    \n",
    "    def log_neo4j_query(self, query_number: int, query: str, params: dict, is_write: bool = False):\n",
    "        \"\"\"Log Neo4j query - separates READ and WRITE operations\"\"\"\n",
    "        file_key = 'neo4j_write' if is_write else 'neo4j'\n",
    "        level = 'NEO4J_WRITE' if is_write else 'NEO4J_QUERY'\n",
    "        \n",
    "        # Detect query type\n",
    "        query_upper = query.strip().upper()\n",
    "        query_type = 'UNKNOWN'\n",
    "        if query_upper.startswith('CREATE'):\n",
    "            query_type = 'CREATE'\n",
    "        elif query_upper.startswith('MERGE'):\n",
    "            query_type = 'MERGE'\n",
    "        elif 'SET ' in query_upper:\n",
    "            query_type = 'UPDATE'\n",
    "        elif query_upper.startswith('DELETE') or 'DELETE ' in query_upper:\n",
    "            query_type = 'DELETE'\n",
    "        elif query_upper.startswith('MATCH'):\n",
    "            query_type = 'READ'\n",
    "        elif 'fulltext.query' in query.lower():\n",
    "            query_type = 'BM25_SEARCH'\n",
    "        elif 'vector.similarity' in query.lower():\n",
    "            query_type = 'VECTOR_SEARCH'\n",
    "        \n",
    "        self._write(file_key, {\n",
    "            'level': level,\n",
    "            'query_number': query_number,\n",
    "            'query_type': query_type,\n",
    "            'query': query,\n",
    "            'params': {k: str(v)[:500] for k, v in params.items()} if params else {},\n",
    "        })\n",
    "    \n",
    "    def log_neo4j_result(self, query_number: int, duration_ms: float, record_count: int, \n",
    "                         records_preview: str, is_write: bool = False):\n",
    "        \"\"\"Log Neo4j query result\"\"\"\n",
    "        file_key = 'neo4j_write' if is_write else 'neo4j'\n",
    "        level = 'NEO4J_WRITE_RESULT' if is_write else 'NEO4J_RESULT'\n",
    "        \n",
    "        self._write(file_key, {\n",
    "            'level': level,\n",
    "            'query_number': query_number,\n",
    "            'duration_ms': duration_ms,\n",
    "            'record_count': record_count,\n",
    "            'records_preview': records_preview,\n",
    "        })\n",
    "    \n",
    "    # ========== Embedding Events ==========\n",
    "    \n",
    "    def log_embedding_start(self, operation: str, input_texts: list[str]):\n",
    "        \"\"\"Log embedding generation start\"\"\"\n",
    "        self.counters['embedding'] += 1\n",
    "        self._write('embeddings', {\n",
    "            'level': 'EMBEDDING_START',\n",
    "            'embedding_number': self.counters['embedding'],\n",
    "            'operation': operation,\n",
    "            'input_count': len(input_texts),\n",
    "            'input_texts': [t[:200] for t in input_texts],  # Truncate for readability\n",
    "        })\n",
    "        return self.counters['embedding']\n",
    "    \n",
    "    def log_embedding_end(self, embedding_number: int, duration_ms: float, output_dims: list[int]):\n",
    "        \"\"\"Log embedding generation result\"\"\"\n",
    "        self._write('embeddings', {\n",
    "            'level': 'EMBEDDING_END',\n",
    "            'embedding_number': embedding_number,\n",
    "            'duration_ms': duration_ms,\n",
    "            'output_dimensions': output_dims,\n",
    "        })\n",
    "    \n",
    "    # ========== Parallel Operation Events ==========\n",
    "    \n",
    "    def log_parallel_start(self, task_group_id: str, task_names: list[str]):\n",
    "        \"\"\"Log start of parallel operation\"\"\"\n",
    "        self.counters['parallel_task'] += 1\n",
    "        self._write('parallel', {\n",
    "            'level': 'PARALLEL_START',\n",
    "            'task_group_id': task_group_id,\n",
    "            'parallel_task_number': self.counters['parallel_task'],\n",
    "            'task_count': len(task_names),\n",
    "            'task_names': task_names,\n",
    "        })\n",
    "        self.active_parallel_tasks[task_group_id] = task_names\n",
    "    \n",
    "    def log_parallel_end(self, task_group_id: str, duration_ms: float, results_summary: str = None):\n",
    "        \"\"\"Log end of parallel operation\"\"\"\n",
    "        task_names = self.active_parallel_tasks.pop(task_group_id, [])\n",
    "        self._write('parallel', {\n",
    "            'level': 'PARALLEL_END',\n",
    "            'task_group_id': task_group_id,\n",
    "            'task_count': len(task_names),\n",
    "            'duration_ms': duration_ms,\n",
    "            'results_summary': results_summary,\n",
    "        })\n",
    "    \n",
    "    # ========== OTEL Span Events ==========\n",
    "    \n",
    "    def log_otel_span_start(self, span_name: str):\n",
    "        \"\"\"Log OTEL span start\"\"\"\n",
    "        self._write('otel', {'level': 'OTEL_SPAN_START', 'span_name': span_name})\n",
    "    \n",
    "    def log_otel_span_end(self, span_name: str, duration_ms: float, attributes: dict = None, status: str = None):\n",
    "        \"\"\"Log OTEL span end\"\"\"\n",
    "        self._write('otel', {\n",
    "            'level': 'OTEL_SPAN_END',\n",
    "            'span_name': span_name,\n",
    "            'duration_ms': duration_ms,\n",
    "            'attributes': attributes,\n",
    "            'status': status,\n",
    "        })\n",
    "    \n",
    "    # ========== Utilities ==========\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close all log files\"\"\"\n",
    "        for f in self.files.values():\n",
    "            f.close()\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get summary of logged events\"\"\"\n",
    "        return {\n",
    "            'counters': self.counters.copy(),\n",
    "            'log_files': [str(self.output_dir / f'{k}.jsonl') for k in self.files.keys()],\n",
    "        }\n",
    "\n",
    "# Initialize the enhanced trace logger\n",
    "trace_logger = EnhancedTraceLogger('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: Configure Logging (Suppress Console Output)\n",
    "#\n",
    "# This configures logging to suppress console output while still\n",
    "# capturing to our trace files.\n",
    "\n",
    "import logging\n",
    "\n",
    "# Suppress all console logging - we only want file output\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    ")\n",
    "\n",
    "# Suppress verbose loggers\n",
    "logging.getLogger('graphiti_core').setLevel(logging.WARNING)\n",
    "logging.getLogger('neo4j').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpcore').setLevel(logging.WARNING)\n",
    "\n",
    "# NO console output - just confirm setup\n",
    "# (This print is the only output from setup cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Deep Trace Hooks (LLM + Neo4j + OTEL)\n",
    "\n",
    "This cell implements **non-invasive** hooks to capture:\n",
    "1. **LLM calls**: Full prompt and response via httpx event hooks\n",
    "2. **Neo4j queries**: Query text, parameters, and results via driver wrapper\n",
    "3. **OTEL spans**: All internal operation spans via custom SpanProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.3: Deep Trace Hooks (File-Only Output)\n",
    "#\n",
    "# These hooks capture complete information WITHOUT modifying graphiti_core source code.\n",
    "# ALL output goes to files - NO console output.\n",
    "#\n",
    "# Captures:\n",
    "# 1. LLM requests/responses via httpx event hooks\n",
    "# 2. Neo4j queries (READ + WRITE) via driver wrapper\n",
    "# 3. OTEL spans via custom SpanProcessor\n",
    "# 4. Embedding generation via TracedEmbedder wrapper\n",
    "\n",
    "import httpx\n",
    "from functools import wraps\n",
    "from opentelemetry.sdk.trace import SpanProcessor\n",
    "from opentelemetry.sdk.trace import ReadableSpan\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LLM Request/Response Hook via httpx (NO console output)\n",
    "# ============================================================================\n",
    "\n",
    "class LLMTraceHook:\n",
    "    \"\"\"Captures LLM requests/responses - file output only.\"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: EnhancedTraceLogger):\n",
    "        self.trace = trace_logger\n",
    "        self.call_count = 0\n",
    "    \n",
    "    async def log_request(self, request: httpx.Request):\n",
    "        \"\"\"Called before each HTTP request\"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        if '/v1/chat/completions' in str(request.url):\n",
    "            try:\n",
    "                body = json.loads(request.content.decode('utf-8'))\n",
    "                self.trace.log_llm_request(\n",
    "                    call_number=self.call_count,\n",
    "                    model=body.get('model'),\n",
    "                    messages=body.get('messages', []),\n",
    "                    temperature=body.get('temperature'),\n",
    "                    max_tokens=body.get('max_tokens'),\n",
    "                )\n",
    "            except Exception:\n",
    "                pass  # Silent fail - no console output\n",
    "    \n",
    "    async def log_response(self, response: httpx.Response):\n",
    "        \"\"\"Called after each HTTP response\"\"\"\n",
    "        if '/v1/chat/completions' in str(response.url):\n",
    "            try:\n",
    "                await response.aread()\n",
    "                body = json.loads(response.content.decode('utf-8'))\n",
    "                choices = body.get('choices', [])\n",
    "                usage = body.get('usage', {})\n",
    "                content = choices[0].get('message', {}).get('content', '') if choices else ''\n",
    "                \n",
    "                self.trace.log_llm_response(\n",
    "                    call_number=self.call_count,\n",
    "                    content=content,\n",
    "                    usage=usage,\n",
    "                    finish_reason=choices[0].get('finish_reason') if choices else None,\n",
    "                )\n",
    "            except Exception:\n",
    "                pass  # Silent fail\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Neo4j Query Hook with WRITE detection (NO console output)\n",
    "# ============================================================================\n",
    "\n",
    "class Neo4jTraceHook:\n",
    "    \"\"\"Wraps Neo4j driver to capture all queries including WRITE operations.\"\"\"\n",
    "    \n",
    "    WRITE_KEYWORDS = ('CREATE', 'MERGE', 'SET ', 'DELETE', 'REMOVE', 'DETACH')\n",
    "    \n",
    "    def __init__(self, trace_logger: EnhancedTraceLogger):\n",
    "        self.trace = trace_logger\n",
    "        self.query_count = 0\n",
    "    \n",
    "    def _is_write_query(self, query: str) -> bool:\n",
    "        \"\"\"Detect if query modifies the database\"\"\"\n",
    "        query_upper = query.strip().upper()\n",
    "        return any(kw in query_upper for kw in self.WRITE_KEYWORDS)\n",
    "    \n",
    "    def wrap_driver(self, driver):\n",
    "        \"\"\"Wrap the driver's execute_query method\"\"\"\n",
    "        original_execute = driver.execute_query\n",
    "        \n",
    "        @wraps(original_execute)\n",
    "        async def traced_execute(cypher_query, **kwargs):\n",
    "            self.query_count += 1\n",
    "            params = kwargs.get('params', {})\n",
    "            is_write = self._is_write_query(cypher_query)\n",
    "            \n",
    "            # Log query (separates READ and WRITE)\n",
    "            self.trace.log_neo4j_query(\n",
    "                query_number=self.query_count,\n",
    "                query=cypher_query,\n",
    "                params=params,\n",
    "                is_write=is_write,\n",
    "            )\n",
    "            \n",
    "            # Execute original\n",
    "            start_time = time.time()\n",
    "            result = await original_execute(cypher_query, **kwargs)\n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Log result\n",
    "            records = result.records if hasattr(result, 'records') else []\n",
    "            record_count = len(records)\n",
    "            \n",
    "            records_preview = []\n",
    "            for r in records[:5]:\n",
    "                try:\n",
    "                    records_preview.append(dict(r))\n",
    "                except:\n",
    "                    records_preview.append(str(r))\n",
    "            \n",
    "            self.trace.log_neo4j_result(\n",
    "                query_number=self.query_count,\n",
    "                duration_ms=duration_ms,\n",
    "                record_count=record_count,\n",
    "                records_preview=str(records_preview)[:1000],\n",
    "                is_write=is_write,\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        driver.execute_query = traced_execute\n",
    "        return driver\n",
    "\n",
    "# ============================================================================\n",
    "# 3. OTEL Span Processor (NO console output)\n",
    "# ============================================================================\n",
    "\n",
    "class TraceSpanProcessor(SpanProcessor):\n",
    "    \"\"\"Custom SpanProcessor - file output only.\"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: EnhancedTraceLogger):\n",
    "        self.trace = trace_logger\n",
    "    \n",
    "    def on_start(self, span: ReadableSpan, parent_context=None):\n",
    "        span_name = span.name if hasattr(span, 'name') else str(span)\n",
    "        self.trace.log_otel_span_start(span_name)\n",
    "    \n",
    "    def on_end(self, span: ReadableSpan):\n",
    "        span_name = span.name\n",
    "        duration_ns = span.end_time - span.start_time if span.end_time and span.start_time else 0\n",
    "        duration_ms = duration_ns / 1_000_000\n",
    "        attributes = dict(span.attributes) if span.attributes else {}\n",
    "        \n",
    "        self.trace.log_otel_span_end(\n",
    "            span_name=span_name,\n",
    "            duration_ms=duration_ms,\n",
    "            attributes={k: str(v) for k, v in attributes.items()},\n",
    "            status=str(span.status) if hasattr(span, 'status') else None,\n",
    "        )\n",
    "    \n",
    "    def shutdown(self):\n",
    "        pass\n",
    "    \n",
    "    def force_flush(self, timeout_millis=None) -> bool:\n",
    "        return True\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TracedEmbedder - Captures all embedding operations\n",
    "# ============================================================================\n",
    "\n",
    "class TracedEmbedder:\n",
    "    \"\"\"Wraps an embedder to log all embedding generation calls.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder, trace_logger: EnhancedTraceLogger):\n",
    "        self.embedder = embedder\n",
    "        self.trace = trace_logger\n",
    "    \n",
    "    async def create(self, input_data):\n",
    "        \"\"\"Wrap single embedding creation\"\"\"\n",
    "        # Normalize input\n",
    "        if isinstance(input_data, str):\n",
    "            texts = [input_data]\n",
    "        elif isinstance(input_data, list) and len(input_data) > 0 and isinstance(input_data[0], str):\n",
    "            texts = input_data\n",
    "        else:\n",
    "            texts = [str(input_data)]\n",
    "        \n",
    "        emb_num = self.trace.log_embedding_start('create', texts)\n",
    "        start = time.time()\n",
    "        \n",
    "        result = await self.embedder.create(input_data)\n",
    "        \n",
    "        duration_ms = (time.time() - start) * 1000\n",
    "        self.trace.log_embedding_end(emb_num, duration_ms, [len(result)])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def create_batch(self, input_data_list: list[str]):\n",
    "        \"\"\"Wrap batch embedding creation\"\"\"\n",
    "        emb_num = self.trace.log_embedding_start('create_batch', input_data_list)\n",
    "        start = time.time()\n",
    "        \n",
    "        results = await self.embedder.create_batch(input_data_list)\n",
    "        \n",
    "        duration_ms = (time.time() - start) * 1000\n",
    "        dims = [len(r) for r in results] if results else []\n",
    "        self.trace.log_embedding_end(emb_num, duration_ms, dims)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Forward other attributes to wrapped embedder\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.embedder, name)\n",
    "\n",
    "# NO print statement - silent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.4: Apply Deep Trace Hooks (Including TracedEmbedder)\n",
    "#\n",
    "# This cell applies ALL hooks to the graphiti instance.\n",
    "# Run AFTER Cell 4.5 (Graphiti initialization) and Cell 5.1 (TraceLogger).\n",
    "#\n",
    "# Applies:\n",
    "# 1. Neo4j trace hook (READ + WRITE detection)\n",
    "# 2. LLM trace hook (httpx event hooks)\n",
    "# 3. OTEL span processor\n",
    "# 4. TracedEmbedder (captures all embedding generation)\n",
    "\n",
    "import httpx\n",
    "from opentelemetry import trace as otel_trace\n",
    "\n",
    "# 1. Apply Neo4j trace hook\n",
    "neo4j_hook = Neo4jTraceHook(trace_logger)\n",
    "neo4j_hook.wrap_driver(graphiti.driver)\n",
    "\n",
    "# 2. Apply LLM trace hook via httpx event hooks\n",
    "llm_hook = LLMTraceHook(trace_logger)\n",
    "traced_http_client = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(LLM_TIMEOUT_SECONDS),\n",
    "    event_hooks={\n",
    "        'request': [llm_hook.log_request],\n",
    "        'response': [llm_hook.log_response],\n",
    "    }\n",
    ")\n",
    "traced_openai_client = AsyncOpenAI(\n",
    "    api_key=local_llm_api_key,\n",
    "    base_url=local_llm_base_url,\n",
    "    timeout=LLM_TIMEOUT_SECONDS,\n",
    "    http_client=traced_http_client,\n",
    ")\n",
    "llm_client.client = traced_openai_client\n",
    "\n",
    "# 3. Add custom span processor to OTEL\n",
    "provider = otel_trace.get_tracer_provider()\n",
    "if hasattr(provider, 'add_span_processor'):\n",
    "    trace_span_processor = TraceSpanProcessor(trace_logger)\n",
    "    provider.add_span_processor(trace_span_processor)\n",
    "\n",
    "# 4. Wrap embedder with TracedEmbedder\n",
    "traced_embedder = TracedEmbedder(embedder, trace_logger)\n",
    "graphiti.clients.embedder = traced_embedder\n",
    "\n",
    "# NO print statements - silent setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Log Files Documentation\n",
    "\n",
    "All trace logs are written to JSONL files in the current directory:\n",
    "\n",
    "| File | Content | Use Case |\n",
    "|------|---------|----------|\n",
    "| `trace_main.jsonl` | High-level operations, API calls | Overview of what happened |\n",
    "| `trace_llm.jsonl` | LLM requests and responses | Full prompts and outputs |\n",
    "| `trace_neo4j.jsonl` | Neo4j READ queries | Retrieval operations |\n",
    "| `trace_neo4j_write.jsonl` | Neo4j WRITE queries (CREATE, MERGE) | Graph modifications |\n",
    "| `trace_embeddings.jsonl` | Embedding generation | Vector creation |\n",
    "| `trace_parallel.jsonl` | Parallel operations | Concurrency analysis |\n",
    "| `trace_otel.jsonl` | OpenTelemetry spans | Performance timing |\n",
    "\n",
    "**Commands to View Logs:**\n",
    "\n",
    "```bash\n",
    "# View main trace\n",
    "cat trace_main.jsonl | jq -c '{ts: .timestamp, event: .event, data: .data}'\n",
    "\n",
    "# View all LLM calls\n",
    "cat trace_llm.jsonl | jq -c 'select(.level==\"LLM_REQUEST\")'\n",
    "\n",
    "# View Neo4j WRITE operations\n",
    "cat trace_neo4j_write.jsonl | jq -c 'select(.level==\"NEO4J_WRITE\")'\n",
    "\n",
    "# View embedding generation\n",
    "cat trace_embeddings.jsonl | jq .\n",
    "\n",
    "# Count operations\n",
    "echo \"LLM calls: $(grep -c LLM_REQUEST trace_llm.jsonl)\"\n",
    "echo \"Neo4j READ: $(grep -c NEO4J_QUERY trace_neo4j.jsonl)\"\n",
    "echo \"Neo4j WRITE: $(grep -c NEO4J_WRITE trace_neo4j_write.jsonl)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ZepSimulator - Simulating Zep Cloud API\n",
    "\n",
    "This class mimics the Zep Python SDK interface using graphiti_core directly.\n",
    "\n",
    "**Search Modes:**\n",
    "- `enable_bfs=False` (default): EDGE_HYBRID_SEARCH_RRF - BM25 + Cosine + RRF\n",
    "- `enable_bfs=True`: EDGE_HYBRID_SEARCH_CROSS_ENCODER - BM25 + Cosine + BFS + vLLM cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: ZepSimulator Class (with BFS Search Support)\n",
    "\n",
    "from graphiti_core.nodes import EpisodeType\n",
    "from graphiti_core.search.search_config_recipes import (\n",
    "    EDGE_HYBRID_SEARCH_RRF,\n",
    "    EDGE_HYBRID_SEARCH_CROSS_ENCODER,\n",
    ")\n",
    "import time\n",
    "\n",
    "# EDGE_HYBRID_SEARCH_CROSS_ENCODER includes BFS and uses the cross_encoder (VLLMRerankerClient)\n",
    "# EDGE_HYBRID_SEARCH_RRF uses only BM25+Cosine with RRF reranking (no graph traversal)\n",
    "\n",
    "class ZepSimulator:\n",
    "    \"\"\"\n",
    "    Simulates Zep Cloud API behavior using graphiti_core directly.\n",
    "    \n",
    "    This provides the same interface patterns as the Zep Python SDK,\n",
    "    allowing us to see exactly what happens internally.\n",
    "    \n",
    "    Mapping:\n",
    "    - add_message() \u2192 zep_client.thread.add_messages() \u2192 graphiti.add_episode()\n",
    "    - get_user_context() \u2192 zep_client.thread.get_user_context() \u2192 graphiti.search()\n",
    "    - graph_search() \u2192 zep_client.graph.search() \u2192 graphiti.search()\n",
    "    \n",
    "    Search Modes:\n",
    "    - enable_bfs=False (default): Uses EDGE_HYBRID_SEARCH_RRF (BM25+Cosine+RRF, no graph traversal)\n",
    "    - enable_bfs=True: Uses EDGE_HYBRID_SEARCH_CROSS_ENCODER (BM25+Cosine+BFS + vLLM cross_encoder)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, graphiti_client: Graphiti, trace_logger: EnhancedTraceLogger):\n",
    "        self.graphiti = graphiti_client\n",
    "        self.trace = trace_logger\n",
    "    \n",
    "    async def add_message(\n",
    "        self,\n",
    "        group_id: str,\n",
    "        role: str,\n",
    "        name: str,\n",
    "        content: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.thread.add_messages()\n",
    "        \n",
    "        This stores a message in the knowledge graph by:\n",
    "        1. Extracting entities from the message (LLM call)\n",
    "        2. Resolving entity duplicates (LLM call)\n",
    "        3. Extracting relationships (LLM call)\n",
    "        4. Creating nodes and edges in Neo4j\n",
    "        \"\"\"\n",
    "        self.trace.log_main('add_message', {\n",
    "            'group_id': group_id,\n",
    "            'role': role,\n",
    "            'name': name,\n",
    "            'content': content,\n",
    "        })\n",
    "        \n",
    "        # Format message as Zep does: \"{role}({role_type}): {content}\"\n",
    "        episode_body = f\"{name}({role}): {content}\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This is the actual graphiti call\n",
    "        # NOTE: Do NOT pass uuid - let graphiti auto-generate it for new episodes.\n",
    "        result = await self.graphiti.add_episode(\n",
    "            name=name,\n",
    "            episode_body=episode_body,\n",
    "            source_description='Agent conversation message',\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "            source=EpisodeType.message,\n",
    "            group_id=group_id,\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        episode_uuid = result.episode.uuid\n",
    "        \n",
    "        self.trace.log_main('add_message_complete', {\n",
    "            'episode_uuid': episode_uuid,\n",
    "            'duration_ms': duration_ms,\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'uuid': episode_uuid,\n",
    "            'duration_ms': duration_ms,\n",
    "            'result': result,\n",
    "        }\n",
    "    \n",
    "    async def get_user_context(\n",
    "        self,\n",
    "        group_id: str,\n",
    "        query: str,\n",
    "        max_facts: int = 10,\n",
    "        enable_bfs: bool = False,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.thread.get_user_context()\n",
    "        \n",
    "        Args:\n",
    "            group_id: The group to search within\n",
    "            query: Natural language query\n",
    "            max_facts: Maximum number of facts to return\n",
    "            enable_bfs: If True, use BFS graph traversal (CROSS_ENCODER config)\n",
    "                       If False, use standard hybrid search (RRF config, default)\n",
    "        \n",
    "        Search configurations:\n",
    "        - enable_bfs=False: EDGE_HYBRID_SEARCH_RRF\n",
    "          - BM25 fulltext search + Cosine vector similarity\n",
    "          - RRF (Reciprocal Rank Fusion) for reranking\n",
    "          - NO graph traversal\n",
    "        \n",
    "        - enable_bfs=True: EDGE_HYBRID_SEARCH_CROSS_ENCODER\n",
    "          - BM25 + Cosine + BFS graph traversal\n",
    "          - Cross-encoder (VLLMRerankerClient) for reranking\n",
    "          - Uses graph structure for path-based retrieval\n",
    "        \"\"\"\n",
    "        # Select search config based on enable_bfs\n",
    "        if enable_bfs:\n",
    "            search_config = EDGE_HYBRID_SEARCH_CROSS_ENCODER\n",
    "            search_mode = 'BFS_CROSS_ENCODER'\n",
    "        else:\n",
    "            search_config = EDGE_HYBRID_SEARCH_RRF\n",
    "            search_mode = 'HYBRID_RRF'\n",
    "        \n",
    "        self.trace.log_main('get_user_context', {\n",
    "            'group_id': group_id,\n",
    "            'query': query,\n",
    "            'max_facts': max_facts,\n",
    "            'enable_bfs': enable_bfs,\n",
    "            'search_mode': search_mode,\n",
    "            'search_config': str(search_config),\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use search_() method which accepts SearchConfig\n",
    "        results = await self.graphiti.search_(\n",
    "            group_ids=[group_id],\n",
    "            query=query,\n",
    "            config=search_config,\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Extract facts from results (limit to max_facts)\n",
    "        # SearchResults has .edges attribute containing EntityEdge objects\n",
    "        facts = [edge.fact for edge in results.edges[:max_facts]]\n",
    "        \n",
    "        self.trace.log_main('get_user_context_complete', {\n",
    "            'duration_ms': duration_ms,\n",
    "            'num_results': len(results.edges),\n",
    "            'facts_returned': len(facts),\n",
    "            'facts': facts,\n",
    "        })\n",
    "        \n",
    "        return facts\n",
    "    \n",
    "    async def graph_search(\n",
    "        self,\n",
    "        group_ids: list[str],\n",
    "        query: str,\n",
    "        max_facts: int = 10,\n",
    "        enable_bfs: bool = False,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.graph.search()\n",
    "        \n",
    "        Same as get_user_context but can search across multiple groups.\n",
    "        \n",
    "        Args:\n",
    "            group_ids: List of groups to search within\n",
    "            query: Natural language query\n",
    "            max_facts: Maximum number of results\n",
    "            enable_bfs: If True, use BFS graph traversal\n",
    "        \"\"\"\n",
    "        if enable_bfs:\n",
    "            search_config = EDGE_HYBRID_SEARCH_CROSS_ENCODER\n",
    "            search_mode = 'BFS_CROSS_ENCODER'\n",
    "        else:\n",
    "            search_config = EDGE_HYBRID_SEARCH_RRF\n",
    "            search_mode = 'HYBRID_RRF'\n",
    "        \n",
    "        self.trace.log_main('graph_search', {\n",
    "            'group_ids': group_ids,\n",
    "            'query': query,\n",
    "            'max_facts': max_facts,\n",
    "            'enable_bfs': enable_bfs,\n",
    "            'search_mode': search_mode,\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = await self.graphiti.search_(\n",
    "            group_ids=group_ids,\n",
    "            query=query,\n",
    "            config=search_config,\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Limit to max_facts - SearchResults has .edges attribute\n",
    "        edges = results.edges[:max_facts]\n",
    "        \n",
    "        self.trace.log_main('graph_search_complete', {\n",
    "            'duration_ms': duration_ms,\n",
    "            'num_results': len(edges),\n",
    "            'facts': [edge.fact for edge in edges],\n",
    "        })\n",
    "        \n",
    "        return edges\n",
    "\n",
    "# Initialize the simulator\n",
    "zep = ZepSimulator(graphiti, trace_logger)\n",
    "# NO print - silent initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent Conversation Demo\n",
    "\n",
    "This demonstrates a real-world conversation that shows:\n",
    "- **Adding nodes**: User introduces themselves\n",
    "- **Adding relationships**: User mentions their work\n",
    "- **Updating**: User provides more information\n",
    "- **Searching**: Agent retrieves context\n",
    "\n",
    "The demo simulates agent_memory_full_example pattern:\n",
    "\n",
    "- Turn 1: User introduces themselves \u2192 Creates Alice Chen, TechCorp entities\n",
    "- Turn 2: User mentions project \u2192 Creates Project Phoenix, LEADS relationship\n",
    "- Turn 3: User provides deadline \u2192 Updates with deadline info\n",
    "- Turn 4: Search test \u2192 Retrieves context about Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Group ID: demo_session_20260203_204107\n",
      "User: Alice Chen\n",
      "Conversation turns: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.1: Define the Conversation\n",
    "\n",
    "# Use a unique group_id for this demo session\n",
    "GROUP_ID = f\"demo_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "USER_NAME = \"Alice Chen\"\n",
    "\n",
    "# The conversation turns\n",
    "CONVERSATION = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"description\": \"User introduces themselves - creates new entities\",\n",
    "        \"user_message\": \"Hi, I'm Alice Chen. I work at TechCorp as a senior software engineer.\",\n",
    "        \"expected_entities\": [\"Alice Chen (Person)\", \"TechCorp (Organization)\"],\n",
    "        \"expected_relationships\": [\"Alice Chen WORKS_AT TechCorp\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"description\": \"User mentions a project - adds more entities\",\n",
    "        \"user_message\": \"I'm currently leading Project Phoenix, which is a cloud migration initiative.\",\n",
    "        \"expected_entities\": [\"Project Phoenix (Project)\"],\n",
    "        \"expected_relationships\": [\"Alice Chen LEADS Project Phoenix\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 3,\n",
    "        \"description\": \"User provides deadline - updates existing entity\",\n",
    "        \"user_message\": \"The project deadline is February 15th, and we have 3 team members.\",\n",
    "        \"expected_entities\": [],\n",
    "        \"expected_relationships\": [\"Project Phoenix HAS_DEADLINE February 15th\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 4,\n",
    "        \"description\": \"Search test - retrieve context about Alice\",\n",
    "        \"search_query\": \"What does Alice work on?\",\n",
    "        \"expected_facts\": [\"Alice works at TechCorp\", \"Alice leads Project Phoenix\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Demo Group ID: {GROUP_ID}\")\n",
    "print(f\"User: {USER_NAME}\")\n",
    "print(f\"Conversation turns: {len(CONVERSATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Hi, I'm Alice Chen. I work at TechCorp as a senior software engineer.\n",
      "\n",
      "Expected entities: ['Alice Chen (Person)', 'TechCorp (Organization)']\n",
      "Expected relationships: ['Alice Chen WORKS_AT TechCorp']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0x701865dc243814cd\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0xad9c71ccdd255c31\",\n",
      "    \"start_time\": \"2026-02-03T12:41:07.299406Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:08.343357Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_message\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0xa08703307cb7a6e8\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0xad9c71ccdd255c31\",\n",
      "    \"start_time\": \"2026-02-03T12:41:08.681976Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:09.882049Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"dedupe_nodes.nodes\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0x39bfde2fab2ec8ae\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0xad9c71ccdd255c31\",\n",
      "    \"start_time\": \"2026-02-03T12:41:09.883844Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:11.448536Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_edges.edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0xbd45d15656ea2f3e\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0xad9c71ccdd255c31\",\n",
      "    \"start_time\": \"2026-02-03T12:41:11.526309Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:11.836363Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0x81bf3cac79295c2a\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0xad9c71ccdd255c31\",\n",
      "    \"start_time\": \"2026-02-03T12:41:11.531220Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:11.851482Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.add_episode\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0xe810d2b2784e8ecd6e12a83c7ed0dcdf\",\n",
      "        \"span_id\": \"0xad9c71ccdd255c31\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2026-02-03T12:41:07.286233Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:11.940350Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"episode.uuid\": \"ef48db72-c855-4e04-b5d2-654f01a240c6\",\n",
      "        \"episode.source\": \"message\",\n",
      "        \"episode.reference_time\": \"2026-02-03T12:41:07.285892+00:00\",\n",
      "        \"group_id\": \"demo_session_20260203_204107\",\n",
      "        \"node.count\": 2,\n",
      "        \"edge.count\": 1,\n",
      "        \"edge.invalidated_count\": 0,\n",
      "        \"previous_episodes.count\": 0,\n",
      "        \"entity_types.count\": 0,\n",
      "        \"edge_types.count\": 0,\n",
      "        \"update_communities\": false,\n",
      "        \"communities.count\": 0,\n",
      "        \"duration_ms\": 4654.248237609863\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "\n",
      "OK: Turn 1 complete. Duration: 4656.3ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.2: Run Turn 1 - User Introduction\n",
    "\n",
    "turn = CONVERSATION[0]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected entities: {turn['expected_entities']}\")\n",
    "print(f\"Expected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Add the user message\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 1 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: I'm currently leading Project Phoenix, which is a cloud migration initiative.\n",
      "\n",
      "Expected entities: ['Project Phoenix (Project)']\n",
      "Expected relationships: ['Alice Chen LEADS Project Phoenix']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x0a8b46424f22ed33\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:11.971473Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:12.808769Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_message\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x25708fa06639d25c\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:12.885921Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:13.589697Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"dedupe_nodes.nodes\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x3fba756aa3e8de67\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:13.592002Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:15.102495Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_edges.edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x670c239d0b2e3369\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:15.172419Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:15.604186Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"dedupe_edges.resolve_edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x4133411b5ff9bc67\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:15.626568Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:15.933170Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x07e9ef109432a7c5\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x30337a89b7e12afe\",\n",
      "    \"start_time\": \"2026-02-03T12:41:15.621586Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:16.068265Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.add_episode\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x2dd356eff4f8e5c5388f8068b81fdc5d\",\n",
      "        \"span_id\": \"0x30337a89b7e12afe\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2026-02-03T12:41:11.961744Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:16.141309Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"episode.uuid\": \"b31f0028-0814-4601-8ec2-ba34c7d8a53d\",\n",
      "        \"episode.source\": \"message\",\n",
      "        \"episode.reference_time\": \"2026-02-03T12:41:11.961544+00:00\",\n",
      "        \"group_id\": \"demo_session_20260203_204107\",\n",
      "        \"node.count\": 2,\n",
      "        \"edge.count\": 1,\n",
      "        \"edge.invalidated_count\": 0,\n",
      "        \"previous_episodes.count\": 1,\n",
      "        \"entity_types.count\": 0,\n",
      "        \"edge_types.count\": 0,\n",
      "        \"update_communities\": false,\n",
      "        \"communities.count\": 0,\n",
      "        \"duration_ms\": 4179.567575454712\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "\n",
      "OK: Turn 2 complete. Duration: 4181.6ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.3: Run Turn 2 - Project Information\n",
    "\n",
    "turn = CONVERSATION[1]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected entities: {turn['expected_entities']}\")\n",
    "print(f\"Expected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 2 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: The project deadline is February 15th, and we have 3 team members.\n",
      "\n",
      "Expected relationships: ['Project Phoenix HAS_DEADLINE February 15th']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0x1444366963704662\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:16.171268Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:17.009229Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_message\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0xe3cd0ca90d2566b2\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:17.076981Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:19.885934Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"medium\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_edges.edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0x424d5de511f2e35e\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:20.055541Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:20.793930Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"dedupe_edges.resolve_edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0xbf9871222d980155\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:20.052370Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:20.825352Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"dedupe_edges.resolve_edge\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0x9f54c41840213f78\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:20.849282Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:21.380917Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.llm.generate\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0x3cab897ec92a12d6\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x34bd59f834b932ee\",\n",
      "    \"start_time\": \"2026-02-03T12:41:20.844603Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:21.500490Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"llm.provider\": \"openai\",\n",
      "        \"model.size\": \"small\",\n",
      "        \"max_tokens\": 4096,\n",
      "        \"prompt.name\": \"extract_nodes.extract_summary\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"zep.graphiti.add_episode\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x775349768cc11717c2976afbfda8b53d\",\n",
      "        \"span_id\": \"0x34bd59f834b932ee\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2026-02-03T12:41:16.161152Z\",\n",
      "    \"end_time\": \"2026-02-03T12:41:21.587108Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"episode.uuid\": \"b6cb0fbd-e771-4dc1-979b-b9bae40f2dc7\",\n",
      "        \"episode.source\": \"message\",\n",
      "        \"episode.reference_time\": \"2026-02-03T12:41:16.160990+00:00\",\n",
      "        \"group_id\": \"demo_session_20260203_204107\",\n",
      "        \"node.count\": 2,\n",
      "        \"edge.count\": 2,\n",
      "        \"edge.invalidated_count\": 0,\n",
      "        \"previous_episodes.count\": 2,\n",
      "        \"entity_types.count\": 0,\n",
      "        \"edge_types.count\": 0,\n",
      "        \"update_communities\": false,\n",
      "        \"communities.count\": 0,\n",
      "        \"duration_ms\": 5425.9233474731445\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"zep-agent-full-trace\",\n",
      "            \"service.version\": \"1.0.0\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "\n",
      "OK: Turn 3 complete. Duration: 5429.0ms\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.4: Run Turn 3 - Deadline Update\n",
    "\n",
    "turn = CONVERSATION[2]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 3 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Query: What does Alice work on?\n",
      "\n",
      "Expected facts: ['Alice works at TechCorp', 'Alice leads Project Phoenix']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "OK: Turn 4 complete. Found 2 facts.\n",
      "\n",
      "Retrieved Facts:\n",
      "   1. Alice Chen is currently leading Project Phoenix.\n",
      "   2. Alice Chen works at TechCorp as a senior software engineer.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.5: Run Turn 4 - Search Test\n",
    "\n",
    "turn = CONVERSATION[3]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nSearch Query: {turn['search_query']}\")\n",
    "print(f\"\\nExpected facts: {turn['expected_facts']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Search for context\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=turn['search_query'],\n",
    "    max_facts=10,\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 4 complete. Found {len(facts)} facts.\")\n",
    "print(\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   TURN 4: Search test - retrieve context about Alice\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: What does Alice work on?\n",
    "\n",
    "# Expected facts: ['Alice works at TechCorp', 'Alice leads Project Phoenix']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: What does Alice work on?\n",
    "#    max_facts: 10\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #36: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #37: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "#       \u2192 3 records, 67.1ms\n",
    "#       \u2192 2 records, 60.2ms\n",
    "# \u2514\u2500 Duration: 93.2ms\n",
    "#    Result: Found 3 facts\n",
    "#  Fact 1\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "#  Fact 2\n",
    "# ...\n",
    "# Retrieved Facts:\n",
    "#    1. Alice Chen is currently leading Project Phoenix.\n",
    "#    2. Project Phoenix has a deadline on February 15th.\n",
    "#    3. Alice Chen works at TechCorp as a senior software engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional Search Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Query: When is the project deadline?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Retrieved Facts:\n",
      "   1. The deadline for Project Phoenix is February 15th.\n",
      "   2. Alice Chen is currently leading Project Phoenix.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.1: Search for project deadline\n",
    "\n",
    "trace_logger.log_section(\"ADDITIONAL SEARCH: Project Deadline\")\n",
    "\n",
    "query = \"When is the project deadline?\"\n",
    "print(f\"\\nSearch Query: {query}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   ADDITIONAL SEARCH: Project Deadline\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: When is the project deadline?\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: When is the project deadline?\n",
    "#    max_facts: 5\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #38: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #39: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: RECORD * 1\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: SUCCESS {'statuses': [{'gql_status': '00000', 'status_description': 'note: successful completion'}], 'type': 'r', 't_last': 1, 'db': 'neo4j'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  C: COMMIT\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  _: <CONNECTION> client state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E86E]  S: SUCCESS {'bookmark': 'FB:kcwQLskJJyV+REC9lj1ew4ZBjkqQ'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E86E]  _: <CONNECTION> server state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.pool | DEBUG | [#E86E]  _: <POOL> released bolt-104875\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: SUCCESS {'bookmark': 'FB:kcwQLskJJyV+REC9lj1ew4ZBjkqQ'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  _: <CONNECTION> server state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.pool | DEBUG | [#E860]  _: <POOL> released bolt-104890\n",
    "# 21:35:27 | graphiti_core.search.search | DEBUG | search returned context for query When is the project deadline? in 89.6604061126709 ms\n",
    "#       \u2192 2 records, 64.8ms\n",
    "#       \u2192 2 records, 55.6ms\n",
    "# \u2514\u2500 Duration: 91.1ms\n",
    "#    Result: Found 2 facts\n",
    "#  Fact 1\n",
    "#    Data: Project Phoenix has a deadline on February 15th.\n",
    "#  Fact 2\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "\n",
    "# Retrieved Facts:\n",
    "#    1. Project Phoenix has a deadline on February 15th.\n",
    "#    2. Alice Chen is currently leading Project Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Query: What company does Alice work for?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Retrieved Facts:\n",
      "   1. The deadline for Project Phoenix is February 15th.\n",
      "   2. Alice Chen works at TechCorp as a senior software engineer.\n",
      "   3. Alice Chen is currently leading Project Phoenix.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.2: Search for company information\n",
    "\n",
    "trace_logger.log_section(\"ADDITIONAL SEARCH: Company Information\")\n",
    "\n",
    "query = \"What company does Alice work for?\"\n",
    "print(f\"\\nSearch Query: {query}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   ADDITIONAL SEARCH: Company Information\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: What company does Alice work for?\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: What company does Alice work for?\n",
    "#    max_facts: 5\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #40: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #41: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "#       \u2192 2 records, 70.1ms\n",
    "#       \u2192 3 records, 60.5ms\n",
    "# 21:35:27 | graphiti_core.search.search | DEBUG | search returned context for query What company does Alice work for? in 95.98636627197266 ms\n",
    "# \u2514\u2500 Duration: 97.9ms\n",
    "#    Result: Found 3 facts\n",
    "#  Fact 1\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "#  Fact 2\n",
    "#    Data: Alice Chen works at TechCorp as a senior software engineer.\n",
    "#  Fact 3\n",
    "#    Data: Project Phoenix has a deadline on February 15th.\n",
    "\n",
    "# Retrieved Facts:\n",
    "#    1. Alice Chen is currently leading Project Phoenix.\n",
    "#    2. Alice Chen works at TechCorp as a senior software engineer.\n",
    "#    3. Project Phoenix has a deadline on February 15th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 BFS Search Comparison\n",
    "\n",
    "This cell demonstrates the difference between standard hybrid search and BFS-enabled search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Query: What projects is Alice working on and what are the deadlines?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[1] Standard Hybrid Search (RRF, no BFS):\n",
      "   1. The deadline for Project Phoenix is February 15th.\n",
      "   2. Alice Chen is currently leading Project Phoenix.\n",
      "   3. Alice Chen works at TechCorp as a senior software engineer.\n",
      "\n",
      "[2] BFS Search (Cross-Encoder with graph traversal):\n",
      "   1. The deadline for Project Phoenix is February 15th.\n",
      "   2. Alice Chen is currently leading Project Phoenix.\n",
      "   3. Alice Chen works at TechCorp as a senior software engineer.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Standard search found: 3 facts\n",
      "BFS search found: 3 facts\n",
      "\n",
      "Check trace_neo4j.jsonl for query differences.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.3: BFS Search Comparison\n",
    "#\n",
    "# This cell demonstrates the difference between standard hybrid search\n",
    "# and BFS-enabled search that uses graph traversal.\n",
    "\n",
    "trace_logger.log_section(\"BFS SEARCH COMPARISON\")\n",
    "\n",
    "query = \"What projects is Alice working on and what are the deadlines?\"\n",
    "print(f\"Search Query: {query}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Standard search (no BFS)\n",
    "print(\"\\n[1] Standard Hybrid Search (RRF, no BFS):\")\n",
    "facts_standard = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    "    enable_bfs=False,  # Default\n",
    ")\n",
    "for i, fact in enumerate(facts_standard):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# BFS-enabled search\n",
    "print(\"\\n[2] BFS Search (Cross-Encoder with graph traversal):\")\n",
    "facts_bfs = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    "    enable_bfs=True,  # Enable BFS\n",
    ")\n",
    "for i, fact in enumerate(facts_bfs):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"Standard search found: {len(facts_standard)} facts\")\n",
    "print(f\"BFS search found: {len(facts_bfs)} facts\")\n",
    "print(\"\\nCheck trace_neo4j.jsonl for query differences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Trace Log Files:\n",
      "   - main.jsonl\n",
      "   - llm.jsonl\n",
      "   - neo4j.jsonl\n",
      "   - neo4j_write.jsonl\n",
      "   - embeddings.jsonl\n",
      "   - parallel.jsonl\n",
      "   - otel.jsonl\n",
      "\n",
      "Operation Counts:\n",
      "   - llm_call: 17\n",
      "   - neo4j_query: 0\n",
      "   - neo4j_write: 0\n",
      "   - embedding: 28\n",
      "   - parallel_task: 0\n",
      "   - otel_span: 0\n",
      "\n",
      "Demo Group ID: demo_session_20260203_204107\n",
      "   - Use this to query the graph directly in Neo4j Browser\n",
      "\n",
      "Neo4j Browser Query:\n",
      "   MATCH (n) WHERE n.group_id = 'demo_session_20260203_204107' RETURN n\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Commands to view logs:\n",
      "   cat trace_main.jsonl | jq -c '{event: .event, data: .data}'\n",
      "   cat trace_llm.jsonl | jq -c 'select(.level==\"LLM_REQUEST\")'\n",
      "   cat trace_neo4j_write.jsonl | jq .\n",
      "   cat trace_embeddings.jsonl | jq .\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.1: Close trace logger and show summary\n",
    "\n",
    "trace_logger.log_section(\"DEMO COMPLETE\")\n",
    "summary = trace_logger.get_summary()\n",
    "trace_logger.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTrace Log Files:\")\n",
    "for log_file in summary['log_files']:\n",
    "    print(f\"   - {log_file}\")\n",
    "\n",
    "print(\"\\nOperation Counts:\")\n",
    "for key, count in summary['counters'].items():\n",
    "    print(f\"   - {key}: {count}\")\n",
    "\n",
    "print(f\"\\nDemo Group ID: {GROUP_ID}\")\n",
    "print(f\"   - Use this to query the graph directly in Neo4j Browser\")\n",
    "\n",
    "print(f\"\\nNeo4j Browser Query:\")\n",
    "print(f\"   MATCH (n) WHERE n.group_id = '{GROUP_ID}' RETURN n\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Commands to view logs:\")\n",
    "print(\"   cat trace_main.jsonl | jq -c '{event: .event, data: .data}'\")\n",
    "print(\"   cat trace_llm.jsonl | jq -c 'select(.level==\\\"LLM_REQUEST\\\")'\")\n",
    "print(\"   cat trace_neo4j_write.jsonl | jq .\")\n",
    "print(\"   cat trace_embeddings.jsonl | jq .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lines of trace_raw.jsonl:\n",
      "\n",
      "  1. [SECTION        ] {\"level\": \"SECTION\", \"title\": \"TURN 1: User introduces themselves - creates new entities\", \"timestam...\n",
      "  2. [API            ] {\"level\": \"API\", \"method\": \"add_message (thread.add_messages)\", \"params\": {\"group_id\": \"demo_session...\n",
      "  3. [GRAPHITI       ] {\"level\": \"GRAPHITI\", \"method\": \"add_episode\", \"params\": {\"group_id\": \"demo_session_20260131_213508\"...\n",
      "  4. [OTEL_SPAN_START] {\"level\": \"OTEL_SPAN_START\", \"span_name\": \"zep.graphiti.add_episode\", \"timestamp\": \"2026-01-31T13:35...\n",
      "  5. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 1, \"query\": \"\\n                                    MATCH (e...\n",
      "  6. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 1, \"duration_ms\": 30.347824096679688, \"record_count\": 0, \"...\n",
      "  7. [OTEL_SPAN_START] {\"level\": \"OTEL_SPAN_START\", \"span_name\": \"zep.graphiti.llm.generate\", \"timestamp\": \"2026-01-31T13:3...\n",
      "  8. [LLM_REQUEST    ] {\"level\": \"LLM_REQUEST\", \"call_number\": 1, \"model\": \"Qwen/Qwen2.5-32B-Instruct\", \"messages\": [{\"role...\n",
      "  9. [LLM_RESPONSE   ] {\"level\": \"LLM_RESPONSE\", \"call_number\": 1, \"content\": \"{\\n  \\\"extracted_entities\\\": [\\n    {\\n     ...\n",
      " 10. [OTEL_SPAN_END  ] {\"level\": \"OTEL_SPAN_END\", \"span_name\": \"zep.graphiti.llm.generate\", \"duration_ms\": 1671.015729, \"at...\n",
      " 11. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 2, \"query\": \"CALL db.index.fulltext.queryNodes(\\\"node_name_...\n",
      " 12. [NEO4J_LOG      ] {\"level\": \"NEO4J_LOG\", \"message\": \"[#E86E]  C: RUN 'CALL db.index.fulltext.queryNodes(\\\"node_name_an...\n",
      " 13. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 3, \"query\": \"\\n                                            ...\n",
      " 14. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 4, \"query\": \"CALL db.index.fulltext.queryNodes(\\\"node_name_...\n",
      " 15. [NEO4J_LOG      ] {\"level\": \"NEO4J_LOG\", \"message\": \"[#E8C4]  C: RUN 'CALL db.index.fulltext.queryNodes(\\\"node_name_an...\n",
      " 16. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 5, \"query\": \"\\n                                            ...\n",
      " 17. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 84.91873741149902, \"record_count\": 0, \"r...\n",
      " 18. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 86.10320091247559, \"record_count\": 0, \"r...\n",
      " 19. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 63.6904239654541, \"record_count\": 0, \"re...\n",
      " 20. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 50.745248794555664, \"record_count\": 0, \"...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.2: View the raw trace log\n",
    "\n",
    "print(\"First 20 lines of trace_raw.jsonl:\\n\")\n",
    "with open('trace_raw.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 20:\n",
    "            print(\"...\")\n",
    "            break\n",
    "        entry = json.loads(line)\n",
    "        print(f\"{i+1:3d}. [{entry.get('level', 'UNKNOWN'):15s}] {json.dumps(entry)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphiti connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.3: Close Graphiti connection\n",
    "\n",
    "await graphiti.close()\n",
    "print(\"Graphiti connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Execution Results\n",
    "\n",
    "This section documents the execution results from actual trace logs. For detailed step-by-step explanations, see `data_flow.md`.\n",
    "\n",
    "### Execution Summary\n",
    "\n",
    "| Turn | Input | Duration | LLM Calls | Neo4j Queries | Created |\n",
    "|------|-------|----------|-----------|---------------|---------|\n",
    "| 1 | \"Hi, I'm Alice Chen. I work at TechCorp...\" | 6813ms | 5 | 10 | Alice Chen, TechCorp, WORKS_AT |\n",
    "| 2 | \"I'm currently leading Project Phoenix...\" | 6273ms | 6 | 10 | Project Phoenix, LEADS_PROJECT |\n",
    "| 3 | \"The deadline for Project Phoenix is Feb 15th...\" | 5881ms | 6 | 15 | PROJECT_DEADLINE |\n",
    "| 4 | Search: \"What does Alice work on?\" | ~130ms | 0 | 2 | - |\n",
    "\n",
    "### Time Breakdown by Component\n",
    "\n",
    "| Component | Turn 1 | Turn 2 | Turn 3 | Turn 4 (Search) |\n",
    "|-----------|--------|--------|--------|-----------------|\n",
    "| LLM Calls | 6106ms (89.6%) | 6009ms (95.8%) | 4835ms (82.2%) | 0ms |\n",
    "| Neo4j Queries | 704ms (10.3%) | 264ms (4.2%) | 1047ms (17.8%) | 127ms (100%) |\n",
    "| **Total** | **6813ms** | **6273ms** | **5881ms** | **~130ms** |\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "| Turn | Key Difference |\n",
    "|------|----------------|\n",
    "| 1 | First episode: all entities/edges are new, no dedup matches |\n",
    "| 2 | Alice Chen found as duplicate \u2192 reuse existing UUID, update summary |\n",
    "| 3 | LEADS_PROJECT detected as duplicate \u2192 episode UUID appended to existing edge |\n",
    "| 4 | Search: no LLM, only parallel BM25 + Cosine queries |\n",
    "\n",
    "### Final Graph State\n",
    "\n",
    "```\n",
    "        TechCorp \u25c4\u2500\u2500WORKS_AT\u2500\u2500 Alice Chen \u2500\u2500LEADS_PROJECT\u2500\u2500\u25ba Project Phoenix\n",
    "                                                                    \u2502\n",
    "                                                           PROJECT_DEADLINE\n",
    "                                                                    \u25bc\n",
    "                                                              (Feb 15th)\n",
    "```\n",
    "\n",
    "**Counts:** 3 Entities, 3 Edges, 3 Episodes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Neo4j's Role\n",
    "\n",
    "To clarify Neo4j's actual role in this architecture:\n",
    "\n",
    "perhaps **Neo4j is NOT a \\\"Graph Operator with LLM\\\"** - it is just a storage engine with vector retrieval capabilities, so zep is kinada like other agent mem papers but a bit better: still a **AI Agent Orchestration Framework Built on Top of a Database**.\n",
    "\n",
    "| Aspect | Neo4j's Role | NOT Neo4j's Role |\n",
    "|--------|--------------|------------------|\n",
    "| Storage | \u2705 Store nodes, edges, embeddings | |\n",
    "| Query | \u2705 Execute Cypher, BM25, vector similarity | |\n",
    "| Reasoning | | \u274c Handled by LLM in Python layer |\n",
    "| Entity Resolution | | \u274c \\\"Alice\\\" vs \\\"Alice Chen\\\" decided by LLM |\n",
    "| Graph Integrity | | \u274c Deduplication logic in Graphiti Core |\n",
    "\n",
    "**Scalability Implications:**\n",
    "\n",
    "- Python/LLM Layer: Processing involves heavy prompt construction, JSON parsing, and concurrent LLM requests. Graphiti Server is the layer that needs horizontal scaling (more Python workers).\n",
    "- DB Layer: Neo4j handles storage and retrieval efficiently. Unless the graph reaches hundreds of millions of nodes, the database is unlikely to be the primary bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Comparison with VikingMem\n",
    "\n",
    "graphiti/zep is NOT a DBMS of agent memory, but VikingMem seems to be a DBMS. The core of zep/graphiti is still python client-side logic, still a hard-coded pipeline, using data shipping. \n",
    "- db usage:\n",
    "  - Yes, zep uses graph db to store all the data including raw data and higher-level 'views', but DB itself has no aware of what it stores.\n",
    "  - However, vikingmem, as I speculate based on the paper, likely has customized the computation/access layer of VikingDB, or perhaps it wraps VikingDB within a tightly integrated system shell.\n",
    "  - No built-in native operators such as SUM/AVG/LLM_MERGE in vikingmem (code/query shipping), no in-system processing, e.g., TTL, TIMECOMPRESS, LLM_MERGE are system primitives. Instead of DB->Python, it modifies the kernel/computation path.\n",
    "- Data model wise: \n",
    "  - Graphiti has schema (entity node, episode node, community node, etc.), but it seems to be descriptive, and the evolutionary logic between nodes is loose (flexible though, fully managed by LLM that may have hallunations);\n",
    "  - However, VikingMem's data model is presciptive and deterministic, as it has defined operators inside the data model. It explicitly states that the attribute Y of Entity X must and can only be calculated from the attribute B of Event A through the operator OP, attempting to make the evolution of memory a deterministic function.\n",
    "- Also, VikingMem has lifecycle management, making Consolidation (memory consolidation/forgetting) a bg automatic process (not explicitly called by the application layer).\n",
    "- Query optimization: graphiti's search is hard-coded python pipeline, but vikingmem mentioned multi-granular indexing and dynamic search, kinda meaning that system can automatically decide the best way to search, and use index effectively (query optimizer?).\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Comparison with LOTUS\n",
    "\n",
    "Target:\n",
    "- LOTUS's target is to save money, sacraficing a little bit accuracy compared to full LLM queries\uff1b\n",
    "- Graphiti's main target is accuracy, not 'saving money'.\n",
    "\n",
    "Methodology:\n",
    "- Graphiti has 2 phases:\n",
    "  - create/update: expensive ETL pipeline, LLM heavy, creating and maintaining the view on top of raw data\n",
    "  - retrieval: only graph query, no LLM, fast, just retrieve the view\n",
    "- LOTUS has 1 phase:\n",
    "  - retrieval: no insertion and ETL, every operation is based on retrieval, LLM heavy, retrieve raw data, then process\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Comparison with Mem0\n",
    "\n",
    "| Feature | Mem0 | Zep |\n",
    "|---------|------|-----|\n",
    "| arch | Python App Server + Vector DB (+ Neo4j for graph) | Python App Server + Neo4j |\n",
    "| raw msg | No | Yes |\n",
    "| mode | pure ETL, only keeps views | ETL + raw data storage |\n",
    "| view | NL facts (+ optional graph triplets) | KG (entity + facts) |\n",
    "| write policy | Extract facts -> Store | Extract Entities + Facts -> Dedup -> Store |\n",
    "| temporal | timestamp only | invalidation supported |\n",
    "\n",
    "Memory/View query policy\n",
    "\n",
    "For **Mem0**:\n",
    "\n",
    "```sql\n",
    "CREATE VIEW user_memory AS\n",
    "SELECT \n",
    "    memory_fact,           -- \"Alice is vegetarian\"\n",
    "    timestamp,\n",
    "    embedding\n",
    "FROM memories\n",
    "WHERE user_id = $user_id\n",
    "  AND semantic_similarity(embedding, $query_embedding) > threshold\n",
    "ORDER BY similarity DESC\n",
    "LIMIT k;\n",
    "\n",
    "-- optional: Graph Memory View (Mem0g)\n",
    "CREATE VIEW user_graph_memory AS\n",
    "SELECT \n",
    "    source_entity,         -- \"Alice\"\n",
    "    relation,              -- \"has_preference\"\n",
    "    target_entity,         -- \"vegetarian\"\n",
    "    timestamp\n",
    "FROM graph_triplets\n",
    "WHERE user_id = $user_id\n",
    "  AND (source_entity SIMILAR TO $query OR target_entity SIMILAR TO $query);\n",
    "```\n",
    "\n",
    "For **Zep**:\n",
    "\n",
    "```sql\n",
    "CREATE VIEW knowledge_graph AS\n",
    "SELECT \n",
    "    e.fact,                -- \"Alice works at TechCorp as a software engineer\"\n",
    "    e.source_node,         -- Entity: Alice\n",
    "    e.target_node,         -- Entity: TechCorp\n",
    "    e.created_at,\n",
    "    e.valid_at,\n",
    "    e.invalid_at           -- support temporal invalidation!\n",
    "FROM edges e\n",
    "WHERE e.group_id = $group_id\n",
    "  AND (\n",
    "    BM25_score(e.fact, $query) > threshold\n",
    "    OR cosine_similarity(e.embedding, $query_embedding) > threshold\n",
    "  )\n",
    "ORDER BY RRF_fusion(BM25_rank, cosine_rank)\n",
    "LIMIT k;\n",
    "\n",
    "-- optional: BFS expansion View\n",
    "CREATE VIEW expanded_knowledge AS\n",
    "SELECT * FROM knowledge_graph\n",
    "UNION\n",
    "SELECT * FROM BFS_traverse(center_node, max_depth=2);\n",
    "\n",
    "CREATE VIEW raw_episodes AS\n",
    "SELECT content, timestamp\n",
    "FROM episodic_nodes\n",
    "WHERE group_id = $group_id\n",
    "ORDER BY created_at DESC\n",
    "LIMIT n;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphiti-neo4j-otel)",
   "language": "python",
   "name": "graphiti-neo4j-otel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}