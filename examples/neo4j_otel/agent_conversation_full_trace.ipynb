{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zep Agent Conversation with Full Trace\n",
    "\n",
    "**Goal**: See every function call, LLM prompt/response, and Neo4j query from user input to database.\n",
    "\n",
    "**Approach**: Use `graphiti_core` directly to simulate Zep Cloud API behavior, with comprehensive tracing.\n",
    "\n",
    "The markdown description is generated by Claude and edited by me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Arch Overview\n",
    "\n",
    "### Zep Cloud Production Architecture\n",
    "\n",
    "In production, Zep Cloud has a multi-layer architecture:\n",
    "\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│   Agent Application                                       │\n",
    "│  (Uses Zep Python SDK: zep_cloud.client)                  │\n",
    "└─────────────────────┬─────────────────────────────────────┘\n",
    "                      │ HTTPS REST API\n",
    "                      ▼\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│  Zep Server (Go)                                          │\n",
    "│  - Manages users, sessions, threads                       │\n",
    "│  - Handles authentication and rate limiting               │\n",
    "│  - Orchestrates memory operations                         │\n",
    "│  Source: zep/legacy/src/api/apihandlers/                  │\n",
    "└─────────────────────┬─────────────────────────────────────┘\n",
    "                      │ HTTP REST API\n",
    "                      ▼\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│  Graphiti Server (Python FastAPI)                         │\n",
    "│  - Wraps graphiti_core library                            │\n",
    "│  - Provides REST endpoints for graph operations           │\n",
    "│  - Handles async processing queue                         │\n",
    "│  Source: zep-graphiti/server/graph_service/               │\n",
    "└─────────────────────┬─────────────────────────────────────┘\n",
    "                      │ Python function calls\n",
    "                      ▼\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│  graphiti_core (Python Library)                           │\n",
    "│  - Temporal knowledge graph engine                        │\n",
    "│  - Entity extraction and resolution                       │\n",
    "│  - Bi-temporal data model                                 │\n",
    "│  Source: zep-graphiti/graphiti_core/                      │\n",
    "├─────────────────────┬─────────────────────────────────────┤\n",
    "│                     │                                     │\n",
    "│    ┌────────────────┴────────────────┐                    │\n",
    "│    ▼                                 ▼                    │\n",
    "│  LLM API                        Neo4j Database            │\n",
    "│  (Entity extraction,            (Graph storage,           │\n",
    "│   summarization,                 Cypher queries)          │\n",
    "│   reranking)                                              │\n",
    "└───────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "like a **Layered Microservices** pattern:\n",
    "\n",
    "| Layer | Component | Role | Responsibility |\n",
    "|-------|-----------|------|----------------|\n",
    "| **Client / Interface** | Zep Python SDK | HTTP Client Wrapper | Converts function calls (`memory.add()`) to REST API requests, handles retries and authentication. **Contains no business logic.** |\n",
    "| **Gateway / Orchestration** | Zep Server (Go) | API Gateway & User Management | Multi-tenant management (AuthN/AuthZ), rate limiting, request routing. Dispatches tasks to downstream services. |\n",
    "| **Worker / Execution** | Graphiti Server (FastAPI) | Async Task Runner | Maintains async job queue. Since LLM processing is slow (seconds to tens of seconds), it converts memory operations into async jobs to prevent blocking the gateway. |\n",
    "| **Business Logic / \\\"The Brain\\\"** | Graphiti Core (Library) | Core Logic Engine | Contains all prompt templates (`prompts/*.py`), LLM interaction flow control (extract → dedupe → validate), and dynamic Cypher query generation (`*_db_queries.py`). **Stateless logic code.** |\n",
    "\n",
    "**Evidence from Source Code:**\n",
    "\n",
    "1. **Zep Python SDK** - HTTP client with no logic:\n",
    "   ```python\n",
    "   # zep/examples/python/graph_example/graph_example.py\n",
    "   client = AsyncZep(api_key=API_KEY)  # Just an HTTP client\n",
    "   await client.graph.add(graph_id=graph_id, data=\"...\", type=\"text\")\n",
    "   ```\n",
    "\n",
    "2. **Zep Server (Go)** - Routes to Graphiti:\n",
    "   ```go\n",
    "   // zep/legacy/src/store/memory_ce.go\n",
    "   graphiti.I().PutMemory(ctx, session.SessionID, memoryMessages.Messages, true)\n",
    "   graphiti.I().GetMemory(ctx, graphiti.GetMemoryRequest{...})\n",
    "   ```\n",
    "\n",
    "3. **Graphiti Server** - Async queue for slow LLM operations:\n",
    "   ```python\n",
    "   # zep-graphiti/server/graph_service/routers/ingest.py\n",
    "   class AsyncWorker:\n",
    "       def __init__(self):\n",
    "           self.queue = asyncio.Queue()  # Async job queue\n",
    "   \n",
    "   @router.post('/messages', status_code=status.HTTP_202_ACCEPTED)  # Returns immediately\n",
    "   async def add_messages(...):\n",
    "       await async_worker.queue.put(partial(add_messages_task, m))  # Queue the job\n",
    "   ```\n",
    "\n",
    "4. **Graphiti Core** - All business logic:\n",
    "   - `prompts/extract_nodes.py` - Entity extraction prompts\n",
    "   - `prompts/dedupe_nodes.py` - Entity deduplication prompts\n",
    "   - `prompts/extract_edges.py` - Relationship extraction prompts\n",
    "   - `models/nodes/node_db_queries.py` - Dynamic Cypher generation (Neo4j/FalkorDB/Kuzu)\n",
    "   - `graphiti.py:add_episode()` - Orchestrates the full extraction pipeline\n",
    "\n",
    "### My Setup\n",
    "\n",
    "I just bypass Zep Server and Graphiti Server, calling `graphiti_core` directly.\n",
    "\n",
    "This can give **complete visibility** into every operation easily\n",
    "\n",
    "later I may run with Zep Server and Graphiti Server, and compare the results.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│  - ZepSimulator: Mimics Zep SDK patterns                   │\n",
    "│  - TraceLogger: Captures all operations                    │\n",
    "│  - Agent: Simple conversation loop                         │\n",
    "└─────────────────────┬──────────────────────────────────────┘\n",
    "                      │ Direct Python calls (OTEL tracing)\n",
    "                      ▼\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│  graphiti_core (Python Library)                            │\n",
    "│  - Full source code access                                 │\n",
    "│  - All internal operations visible                         │\n",
    "├─────────────────────┬──────────────────────────────────────┤\n",
    "│    ┌────────────────┴────────────────┐                     │\n",
    "│    ▼                                 ▼                     │\n",
    "│  vLLM Server                    Local Neo4j                │\n",
    "│  (Qwen2.5-32B on H100)          (bolt://localhost:7687)    │\n",
    "│  - HTTP traffic logged          - All queries logged       │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zep Cloud API → Graphiti Complete Mapping\n",
    "\n",
    "This mapping is verified from source code analysis.\n",
    "\n",
    "### API Mapping Table\n",
    "\n",
    "| Zep Python SDK | Zep Server (Go) | Graphiti Server | graphiti_core | Purpose |\n",
    "|----------------|-----------------|-----------------|---------------|----------|\n",
    "| `thread.add_messages()` | `memory_ce.go:_initializeProcessingMemory()` | `POST /messages` | `add_episode()` | Store conversation messages |\n",
    "| `thread.get_user_context()` | `memory_ce.go:_get()` | `POST /get-memory` | `search()` | Retrieve relevant context |\n",
    "| `session.get_memory()` | `memory_ce.go:_get()` | `POST /get-memory` | `search()` | Get session memory |\n",
    "| `graph.search()` | `memory_ce.go:_searchSessions()` | `POST /search` | `search()` | Search knowledge graph |\n",
    "| `user.add()` | `userstore_ce.go:_processCreatedUser()` | `POST /entity-node` | `save_entity_node()` | Create user entity |\n",
    "| `graph.add()` | N/A | `POST /entity-node` | `save_entity_node()` | Add structured data |\n",
    "| `user.delete()` | `user_handlers.go:DeleteUserHandler()` | `DELETE /group/{group_id}` | `delete_group()` | Delete user and all data |\n",
    "| `fact.get()` | `fact_handlers_ce.go:getFact()` | `GET /entity-edge/{uuid}` | `EntityEdge.get_by_uuid()` | Get a specific fact |\n",
    "| `fact.delete()` | `fact_handlers_ce.go:deleteSessionFact()` | `DELETE /entity-edge/{uuid}` | `delete_entity_edge()` | Delete a fact |\n",
    "| `session.delete_memory()` | `memory_handlers_ce.go:deleteMemory()` | `DELETE /group/{group_id}` | `delete_group()` | Delete session memory |\n",
    "| `episode.delete()` | N/A | `DELETE /episode/{uuid}` | `delete_episodic_node()` | Delete an episode |\n",
    "\n",
    "### Source Code Evidence\n",
    "\n",
    "> **Just FYI, no need to read this.**\n",
    "\n",
    "**1. thread.add_messages() → add_episode()**\n",
    "\n",
    "From `zep/legacy/src/store/memory_ce.go`:\n",
    "```go\n",
    "func (dao *memoryDAO) _initializeProcessingMemory(...) error {\n",
    "    err := graphiti.I().PutMemory(ctx, session.SessionID, memoryMessages.Messages, true)\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/ingest.py`:\n",
    "```python\n",
    "@router.post('/messages', status_code=status.HTTP_202_ACCEPTED)\n",
    "async def add_messages(request: AddMessagesRequest, graphiti: ZepGraphitiDep):\n",
    "    async def add_messages_task(m: Message):\n",
    "        await graphiti.add_episode(\n",
    "            uuid=m.uuid,\n",
    "            group_id=request.group_id,\n",
    "            name=m.name,\n",
    "            episode_body=f'{m.role or \"\"}({m.role_type}): {m.content}',\n",
    "            ...\n",
    "        )\n",
    "```\n",
    "\n",
    "**2. thread.get_user_context() → search()**\n",
    "\n",
    "From `zep/legacy/src/store/memory_ce.go`:\n",
    "```go\n",
    "func (dao *memoryDAO) _get(...) (*models.Memory, error) {\n",
    "    memory, err := graphiti.I().GetMemory(ctx, graphiti.GetMemoryRequest{\n",
    "        GroupID:  groupID,\n",
    "        MaxFacts: 5,\n",
    "        Messages: mForRetrieval,\n",
    "    })\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/retrieve.py`:\n",
    "```python\n",
    "@router.post('/get-memory', status_code=status.HTTP_200_OK)\n",
    "async def get_memory(request: GetMemoryRequest, graphiti: ZepGraphitiDep):\n",
    "    combined_query = compose_query_from_messages(request.messages)\n",
    "    result = await graphiti.search(\n",
    "        group_ids=[request.group_id],\n",
    "        query=combined_query,\n",
    "        num_results=request.max_facts,\n",
    "    )\n",
    "```\n",
    "\n",
    "**3. user.delete() → delete_group()**\n",
    "\n",
    "From `zep/legacy/src/api/apihandlers/user_handlers.go`:\n",
    "```go\n",
    "func DeleteUserHandler(appState *models.AppState) http.HandlerFunc {\n",
    "    return func(w http.ResponseWriter, r *http.Request) {\n",
    "        err := userStore.DeleteUser(ctx, userID)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/zep_graphiti.py`:\n",
    "```python\n",
    "async def delete_group(self, group_id: str):\n",
    "    edges = await EntityEdge.get_by_group_ids(self.driver, [group_id])\n",
    "    for edge in edges:\n",
    "        await edge.delete(self.driver)\n",
    "    # Also deletes nodes and episodes\n",
    "```\n",
    "\n",
    "**4. fact.get() / fact.delete() → get_entity_edge() / delete_entity_edge()**\n",
    "\n",
    "From `zep/legacy/src/api/apihandlers/fact_handlers_ce.go`:\n",
    "```go\n",
    "func getFact(...) (*models.Fact, error) { ... }\n",
    "func deleteSessionFact(...) error { ... }\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/ingest.py`:\n",
    "```python\n",
    "@router.delete('/entity-edge/{uuid}')\n",
    "async def delete_entity_edge(uuid: str, graphiti: ZepGraphitiDep):\n",
    "    await graphiti.delete_entity_edge(uuid)\n",
    "```\n",
    "\n",
    "From `zep-graphiti/server/graph_service/routers/retrieve.py`:\n",
    "```python\n",
    "@router.get('/entity-edge/{uuid}')\n",
    "async def get_entity_edge(uuid: str, graphiti: ZepGraphitiDep):\n",
    "    return await graphiti.get_entity_edge(uuid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Internal Flow: What Happens Inside Each Operation\n",
    "\n",
    "### 3.1 add_episode() Internal Flow\n",
    "\n",
    "When you call `graphiti.add_episode()`, here's what happens internally.\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant Agent\n",
    "    participant ZepAPI as Zep Server (Go)\n",
    "    participant Worker as Graphiti Server (Py)\n",
    "    participant LLM\n",
    "    participant Neo4j\n",
    "\n",
    "    Note over Agent, ZepAPI: Phase 1: Ingest (Fast)\n",
    "    Agent->>ZepAPI: POST \"I live in SF\"\n",
    "    ZepAPI->>DB: Save Raw Msg\n",
    "    ZepAPI->>Worker: Enqueue Task\n",
    "    ZepAPI-->>Agent: 202 Accepted\n",
    "\n",
    "    Note over Worker, Neo4j: Phase 2: Async ETL (Slow)\n",
    "    Worker->>LLM: Extract Entities (\"SF\")\n",
    "    LLM-->>Worker: JSON\n",
    "    Worker->>Neo4j: Search Existing (\"San Francisco\"?)\n",
    "    Neo4j-->>Worker: Candidates\n",
    "    Worker->>LLM: Deduplicate (Is \"SF\" == \"San Francisco\"?)\n",
    "    LLM-->>Worker: Yes\n",
    "    Worker->>LLM: Extract Relation (LIVES_IN)\n",
    "    LLM-->>Worker: Edge Data\n",
    "    Worker->>Neo4j: COMMIT Transaction (Nodes + Edges)\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "add_episode(episode_body=\"Alice Chen(user): Hi, I'm Alice Chen. I work at TechCorp...\")\n",
    "│\n",
    "├─► Step 0: Query Previous Episodes\n",
    "│   Neo4j: MATCH (e:Episodic) WHERE e.valid_at <= $reference_time AND e.group_id IN $group_ids\n",
    "│          RETURN e ORDER BY e.valid_at DESC LIMIT $num_episodes\n",
    "│   Purpose: Get conversation context for entity extraction\n",
    "│\n",
    "├─► Step 1: Extract Entities (LLM Call - extract_nodes.extract_message)\n",
    "│   System: \"You are an AI assistant that extracts entity nodes from conversational messages...\"\n",
    "│   User: \"<ENTITY TYPES>...</ENTITY TYPES> <CURRENT MESSAGE>Alice Chen(user): Hi, I'm Alice Chen...</CURRENT MESSAGE>\"\n",
    "│   Response: {\"extracted_entities\": [{\"name\": \"Alice Chen\", \"entity_type_id\": 0}, {\"name\": \"TechCorp\", \"entity_type_id\": 0}]}\n",
    "│\n",
    "├─► Step 2: Search for Existing Entities (Neo4j - Parallel Queries)\n",
    "│   For EACH extracted entity, run two parallel searches:\n",
    "│   │\n",
    "│   ├─► 2a: BM25 Fulltext Search\n",
    "│   │   Neo4j: CALL db.index.fulltext.queryNodes(\\\"node_name_and_summary\\\", $query, {limit: $limit})\n",
    "│   │          YIELD node AS n, score WHERE n.group_id IN $group_ids\n",
    "│   │\n",
    "│   └─► 2b: Cosine Similarity Search\n",
    "│       Neo4j: MATCH (n:Entity) WHERE n.group_id IN $group_ids\n",
    "│              WITH n, vector.similarity.cosine(n.name_embedding, $search_vector) AS score\n",
    "│              WHERE score > $min_score RETURN n ORDER BY score DESC\n",
    "│\n",
    "├─► Step 3: Deduplicate Entities (LLM Call - dedupe_nodes.nodes)\n",
    "│   System: \"You are a helpful assistant that determines whether or not ENTITIES extracted from a conversation are duplicates...\"\n",
    "│   User: \"<ENTITIES>[extracted entities]</ENTITIES> <EXISTING ENTITIES>[candidates from Neo4j]</EXISTING ENTITIES>\"\n",
    "│   Response: {\"entity_resolutions\": [{\"id\": 0, \"name\": \"Alice Chen\", \"duplicate_idx\": -1, \"duplicates\": []}, ...]}\n",
    "│   Purpose: Match new entities to existing ones or confirm they are new\n",
    "│\n",
    "├─► Step 4: Extract Relationships (LLM Call - extract_edges.edge)\n",
    "│   System: \"You are an expert fact extractor that extracts fact triples from text...\"\n",
    "│   User: \"<ENTITIES>[resolved entities]</ENTITIES> <CURRENT_MESSAGE>...</CURRENT_MESSAGE> <REFERENCE_TIME>...</REFERENCE_TIME>\"\n",
    "│   Response: {\"edges\": [{\"relation_type\": \"WORKS_AT\", \"source_entity_id\": 0, \"target_entity_id\": 1,\n",
    "│              \"fact\": \"Alice Chen works at TechCorp as a senior software engineer.\", \"valid_at\": \"2026-01-31T13:35:08Z\"}]}\n",
    "│\n",
    "├─► Step 5: Search for Existing Edges (Neo4j - Multiple Queries)\n",
    "│   │\n",
    "│   ├─► 5a: Direct Edge Lookup\n",
    "│   │   Neo4j: MATCH (n:Entity {uuid: $source_node_uuid})-[e:RELATES_TO]->(m:Entity {uuid: $target_node_uuid}) RETURN e\n",
    "│   │\n",
    "│   ├─► 5b: BM25 Fulltext Search on Edges\n",
    "│   │   Neo4j: CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", $query, {limit: $limit})\n",
    "│   │          YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_TO {uuid: rel.uuid}]->(m:Entity)\n",
    "│   │\n",
    "│   └─► 5c: Cosine Similarity Search on Edges\n",
    "│       Neo4j: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids\n",
    "│              WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_vector) AS score\n",
    "│\n",
    "├─► Step 6: Deduplicate Edges (LLM Call - dedupe_edges.resolve_edge) [Only if existing edges found]\n",
    "│   System: \"You are a helpful assistant that de-duplicates facts from fact lists...\"\n",
    "│   User: \"<EXISTING FACTS>[...]</EXISTING FACTS> <FACT INVALIDATION CANDIDATES>[...]</FACT INVALIDATION CANDIDATES> <NEW FACT>...</NEW FACT>\"\n",
    "│   Response: {\"duplicate_facts\": [], \"contradicted_facts\": [], \"fact_type\": \"DEFAULT\"}\n",
    "│   Purpose: Detect duplicate or contradicting facts\n",
    "│\n",
    "├─► Step 7: Generate Entity Summaries (LLM Calls - PARALLEL - extract_nodes.extract_summary)\n",
    "│   For EACH new or updated entity, run in parallel:\n",
    "│   System: \"You are a helpful assistant that extracts entity summaries from the provided text...\"\n",
    "│   User: \"<MESSAGES>[conversation history]</MESSAGES> <ENTITY>{name, summary, entity_types, attributes}</ENTITY>\"\n",
    "│   Response: {\"summary\": \"Alice Chen works at TechCorp as a senior software engineer.\"}\n",
    "│   parallel execution\n",
    "│\n",
    "└─► Step 8: Write to Neo4j (Implicit - happens during steps above)\n",
    "    Entities and edges are created/updated as they are processed.\n",
    "    The EpisodicNode is also created to store the original message.\n",
    "```\n",
    "\n",
    "### 3.2 search() Internal Flow\n",
    "\n",
    "When you call `graphiti.search()`, the system uses **two search methods in parallel** for edge retrieval.\n",
    "\n",
    "```\n",
    "search(query=\"What does Alice work on?\", group_ids=[\"demo_session_...\"], num_results=10)\n",
    "│\n",
    "├─► Step 1: Generate Query Embedding (Local Embedder)\n",
    "│   Embedder: sentence-transformers/all-MiniLM-L6-v2\n",
    "│   encode(\"What does Alice work on?\") → [0.12, -0.34, ...] (384 dimensions)\n",
    "│\n",
    "├─► Step 2: Execute Search Methods (Parallel)\n",
    "│   │\n",
    "│   ├─► 2a: BM25 Fulltext Search on Relationships\n",
    "│   │   Neo4j: CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", $query, {limit: $limit})\n",
    "│   │          YIELD relationship AS rel, score\n",
    "│   │          MATCH (n:Entity)-[e:RELATES_TO {uuid: rel.uuid}]->(m:Entity)\n",
    "│   │          WHERE e.group_id IN $group_ids\n",
    "│   │          RETURN e, n, m ORDER BY score DESC\n",
    "│   │\n",
    "│   └─► 2b: Cosine Similarity Search on Relationships\n",
    "│       Neo4j: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity)\n",
    "│              WHERE e.group_id IN $group_ids\n",
    "│              WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_vector) AS score\n",
    "│              WHERE score > $min_score\n",
    "│              RETURN e, n, m ORDER BY score DESC LIMIT $limit\n",
    "│\n",
    "├─► Step 3: Combine and Deduplicate Results\n",
    "│   Merge results from both search methods\n",
    "│   Remove duplicates based on edge UUID\n",
    "│   Sort by relevance score\n",
    "│\n",
    "└─► Step 4: Return Top Results\n",
    "    Return: [EntityEdge(fact=\"Alice Chen is currently leading Project Phoenix.\", ...),\n",
    "             EntityEdge(fact=\"Project Phoenix has a deadline on February 15th.\", ...),\n",
    "             EntityEdge(fact=\"Alice Chen works at TechCorp as a senior software engineer.\", ...)]\n",
    "```\n",
    "\n",
    "**Search Methods by Data Type** (from `graphiti_core/search/search.py`):\n",
    "- **EntityEdge**: BM25 Fulltext + Cosine Similarity (+ optional BFS, LLM rerank)\n",
    "- **EntityNode**: BM25 Fulltext + Cosine Similarity (+ optional BFS)\n",
    "- **EpisodicNode**: BM25 Fulltext only\n",
    "- **Community**: BM25 Fulltext + Cosine Similarity\n",
    "\n",
    "### 3.3 Why No BFS in This Demo?\n",
    "\n",
    "You may notice that **BFS** is not used in this demo's search. This is **by configuration, not because BFS is unimplemented**.\n",
    "\n",
    "**How Search Configuration Works:**\n",
    "\n",
    "1. **BFS is fully implemented** in `graphiti_core/search/search_utils.py`:\n",
    "   - `edge_bfs_search()` - BFS traversal to find edges from origin nodes\n",
    "   - `node_bfs_search()` - BFS traversal to find nodes from origin nodes\n",
    "\n",
    "2. **BFS is an optional search method** defined in `search_config.py`:\n",
    "   ```python\n",
    "   class EdgeSearchMethod(Enum):\n",
    "       cosine_similarity = 'cosine_similarity'\n",
    "       bm25 = 'bm25'\n",
    "       bfs = 'breadth_first_search'  # Optional!\n",
    "   ```\n",
    "\n",
    "3. **`graphiti.search()` uses `EDGE_HYBRID_SEARCH_RRF` by default** (from `graphiti.py`):\n",
    "   ```python\n",
    "   # In graphiti.search():\n",
    "   search_config = EDGE_HYBRID_SEARCH_RRF  # When center_node_uuid is None\n",
    "   ```\n",
    "\n",
    "4. **`EDGE_HYBRID_SEARCH_RRF` does NOT include BFS** (from `search_config_recipes.py`):\n",
    "   ```python\n",
    "   EDGE_HYBRID_SEARCH_RRF = SearchConfig(\n",
    "       edge_config=EdgeSearchConfig(\n",
    "           search_methods=[EdgeSearchMethod.bm25, EdgeSearchMethod.cosine_similarity],\n",
    "           # Note: NO EdgeSearchMethod.bfs here!\n",
    "           reranker=EdgeReranker.rrf,\n",
    "       )\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**To Enable BFS**, use `search_()` with a config that includes BFS:\n",
    "\n",
    "```python\n",
    "from graphiti_core.search.search_config_recipes import EDGE_HYBRID_SEARCH_CROSS_ENCODER\n",
    "\n",
    "# Option 1: Use pre-defined config with BFS\n",
    "results = await graphiti.search_(\n",
    "    query=\"What does Alice work on?\",\n",
    "    config=EDGE_HYBRID_SEARCH_CROSS_ENCODER,  # Includes BFS!\n",
    "    group_ids=[group_id],\n",
    "    bfs_origin_node_uuids=[\"alice_node_uuid\"],  # Optional: specify start nodes\n",
    ")\n",
    "\n",
    "# Option 2: Custom config\n",
    "from graphiti_core.search.search_config import EdgeSearchConfig, EdgeSearchMethod, SearchConfig, EdgeReranker\n",
    "\n",
    "custom_config = SearchConfig(\n",
    "    edge_config=EdgeSearchConfig(\n",
    "        search_methods=[\n",
    "            EdgeSearchMethod.bm25,\n",
    "            EdgeSearchMethod.cosine_similarity,\n",
    "            EdgeSearchMethod.bfs,  # Add BFS!\n",
    "        ],\n",
    "        reranker=EdgeReranker.rrf,\n",
    "        bfs_max_depth=2,  # How deep to traverse\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**When to Use BFS:**\n",
    "- When you have a known starting node (e.g., \\\"Alice Chen\\\") and want to explore related facts\n",
    "- For graph traversal queries like \\\"Find all facts connected to this entity\\\"\n",
    "- When semantic similarity alone may miss structurally related information\n",
    "\n",
    "**Available Search Config Recipes** (from `search_config_recipes.py`):\n",
    "\n",
    "| Config Name | Search Methods | Reranker | BFS? |\n",
    "|-------------|----------------|----------|------|\n",
    "| `EDGE_HYBRID_SEARCH_RRF` | BM25 + Cosine | RRF | N |\n",
    "| `EDGE_HYBRID_SEARCH_MMR` | BM25 + Cosine | MMR | N |\n",
    "| `EDGE_HYBRID_SEARCH_CROSS_ENCODER` | BM25 + Cosine + **BFS** | Cross-Encoder | Y |\n",
    "| `COMBINED_HYBRID_SEARCH_CROSS_ENCODER` | BM25 + Cosine + **BFS** | Cross-Encoder | Y |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Setup\n",
    "\n",
    "The following cells configure the same environment as `graphiti_neo4j_otel_demo.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Jan 14 2026, 19:35:58) [Clang 21.1.4 ]\n",
      "Working dir: /mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.1: Imports and Basic Logging Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timezone\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j URI: bolt://localhost:7687\n",
      "Local LLM Enabled: True\n",
      "  Base URL: http://localhost:8801/v1\n",
      "  Model: Qwen/Qwen2.5-32B-Instruct\n",
      "Embedding Provider: local\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.2: Load Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j Configuration\n",
    "neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# Local LLM Configuration\n",
    "local_llm_enabled = os.environ.get('LOCAL_LLM_ENABLED', 'false').lower() == 'true'\n",
    "local_llm_base_url = os.environ.get('LOCAL_LLM_BASE_URL', 'http://localhost:8000/v1')\n",
    "local_llm_model = os.environ.get('LOCAL_LLM_MODEL', 'Qwen/Qwen2.5-32B-Instruct')\n",
    "local_llm_api_key = os.environ.get('LOCAL_LLM_API_KEY', 'vllm')\n",
    "\n",
    "# Embedding Configuration\n",
    "embedding_provider = os.environ.get('EMBEDDING_PROVIDER', 'local')\n",
    "local_embedding_model = os.environ.get('LOCAL_EMBEDDING_MODEL', 'all-MiniLM-L6-v2')\n",
    "\n",
    "print(f'Neo4j URI: {neo4j_uri}')\n",
    "print(f'Local LLM Enabled: {local_llm_enabled}')\n",
    "if local_llm_enabled:\n",
    "    print(f'  Base URL: {local_llm_base_url}')\n",
    "    print(f'  Model: {local_llm_model}')\n",
    "print(f'Embedding Provider: {embedding_provider}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry tracing configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.3: Configure OpenTelemetry Tracing\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "\n",
    "def setup_otel_tracing():\n",
    "    \"\"\"Configure OpenTelemetry to output traces to console\"\"\"\n",
    "    resource = Resource(attributes={\n",
    "        'service.name': 'zep-agent-full-trace',\n",
    "        'service.version': '1.0.0',\n",
    "    })\n",
    "    \n",
    "    provider = TracerProvider(resource=resource)\n",
    "    provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "    trace.set_tracer_provider(provider)\n",
    "    \n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "otel_tracer = setup_otel_tracing()\n",
    "print(\"OpenTelemetry tracing configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformerEmbedder class defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.4: Define Local Embedder (sentence-transformers)\n",
    "from graphiti_core.embedder.client import EmbedderClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceTransformerEmbedder(EmbedderClient):\n",
    "    \"\"\"\n",
    "    Local embedder using sentence-transformers.\n",
    "    No API key required - runs entirely locally.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        print(f\"Loading sentence-transformers model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Model loaded. Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    async def create(\n",
    "        self, input_data: str | list[str] | Iterable[int] | Iterable[Iterable[int]]\n",
    "    ) -> list[float]:\n",
    "        \"\"\"Create embedding for input text.\"\"\"\n",
    "        if isinstance(input_data, str):\n",
    "            text = input_data\n",
    "        elif isinstance(input_data, list) and len(input_data) > 0:\n",
    "            text = input_data[0] if isinstance(input_data[0], str) else str(input_data[0])\n",
    "        else:\n",
    "            text = str(input_data)\n",
    "        \n",
    "        loop = asyncio.get_running_loop()\n",
    "        embedding = await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "        )\n",
    "        return embedding\n",
    "    \n",
    "    async def create_batch(self, input_data_list: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Create embeddings for a batch of texts.\"\"\"\n",
    "        loop = asyncio.get_running_loop()\n",
    "        embeddings = await loop.run_in_executor(\n",
    "            None,\n",
    "            lambda: self.model.encode(input_data_list, convert_to_numpy=True).tolist()\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "print(\"SentenceTransformerEmbedder class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing local embedder with model: all-MiniLM-L6-v2\n",
      "Loading sentence-transformers model: all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Using Local LLM at http://localhost:8801/v1\n",
      "Model: Qwen/Qwen2.5-32B-Instruct\n",
      "Timeout: 600s\n",
      "Graphiti initialized, connected to bolt://localhost:7687\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.5: Initialize Graphiti\n",
    "#\n",
    "# IMPORTANT: Make sure vLLM is running on port 8801:\n",
    "# CUDA_VISIBLE_DEVICES=4,5 uv run vllm serve Qwen/Qwen2.5-32B-Instruct \\\n",
    "#     --port 8801 --api-key vllm --tensor-parallel-size 2 \\\n",
    "#     --max-model-len 16384 --enforce-eager --gpu-memory-utilization 0.85\n",
    "\n",
    "from graphiti_core import Graphiti\n",
    "from graphiti_core.llm_client.config import LLMConfig\n",
    "from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient\n",
    "from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Timeout Configuration\n",
    "# NOTE: add_episode() makes multiple LLM calls (entity extraction, resolution, etc.)\n",
    "# Each call can take 30-60+ seconds with local vLLM, so we need a generous timeout.\n",
    "LLM_TIMEOUT_SECONDS = 600  # 10 minutes per LLM call\n",
    "\n",
    "# Initialize Local Embedder\n",
    "print(f'Initializing local embedder with model: {local_embedding_model}')\n",
    "embedder = SentenceTransformerEmbedder(model_name=local_embedding_model)\n",
    "\n",
    "# Initialize LLM Client\n",
    "if not local_llm_enabled:\n",
    "    raise ValueError('LOCAL_LLM_ENABLED must be true in .env')\n",
    "\n",
    "print(f'Using Local LLM at {local_llm_base_url}')\n",
    "print(f'Model: {local_llm_model}')\n",
    "print(f'Timeout: {LLM_TIMEOUT_SECONDS}s')\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    api_key=local_llm_api_key,\n",
    "    model=local_llm_model,\n",
    "    small_model=local_llm_model,\n",
    "    base_url=local_llm_base_url,\n",
    ")\n",
    "\n",
    "custom_openai_client = AsyncOpenAI(\n",
    "    api_key=local_llm_api_key,\n",
    "    base_url=local_llm_base_url,\n",
    "    timeout=LLM_TIMEOUT_SECONDS,\n",
    ")\n",
    "\n",
    "llm_client = OpenAIGenericClient(\n",
    "    config=llm_config,\n",
    "    client=custom_openai_client,\n",
    "    max_tokens=4096\n",
    ")\n",
    "cross_encoder = OpenAIRerankerClient(client=llm_client, config=llm_config)\n",
    "\n",
    "# Initialize Graphiti\n",
    "graphiti = Graphiti(\n",
    "    uri=neo4j_uri,\n",
    "    user=neo4j_user,\n",
    "    password=neo4j_password,\n",
    "    llm_client=llm_client,\n",
    "    embedder=embedder,\n",
    "    cross_encoder=cross_encoder,\n",
    "    tracer=otel_tracer,\n",
    "    trace_span_prefix='zep.graphiti',\n",
    ")\n",
    "\n",
    "print(f'Graphiti initialized, connected to {neo4j_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices and constraints built successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.6: Build Indices and Constraints\n",
    "await graphiti.build_indices_and_constraints()\n",
    "print(\"Indices and constraints built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trace System Implementation\n",
    "\n",
    "We implement a dual-output trace system:\n",
    "- **Raw JSON** → `trace_raw.jsonl` (for debugging)\n",
    "- **Pretty Print** → stdout (for reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TraceLogger initialized. Raw logs: /mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel/trace_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: TraceLogger - Dual Output Trace System\n",
    "\n",
    "class TraceLogger:\n",
    "    \"\"\"\n",
    "    Captures and formats trace information at all levels.\n",
    "    \n",
    "    Output:\n",
    "    - Raw JSON to file (for debugging)\n",
    "    - Pretty-printed text to stdout (for reading)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str = 'trace_raw.jsonl'):\n",
    "        self.log_file = Path(log_file)\n",
    "        self.raw_log = open(self.log_file, 'w')\n",
    "        self.indent_level = 0\n",
    "        print(f\"TraceLogger initialized. Raw logs: {self.log_file.absolute()}\")\n",
    "    \n",
    "    def _write_raw(self, entry: dict):\n",
    "        \"\"\"Write raw JSON to log file\"\"\"\n",
    "        entry['timestamp'] = datetime.now(timezone.utc).isoformat()\n",
    "        self.raw_log.write(json.dumps(entry) + '\\n')\n",
    "        self.raw_log.flush()\n",
    "    \n",
    "    def _indent(self) -> str:\n",
    "        return '   ' * self.indent_level\n",
    "    \n",
    "    def log_section(self, title: str):\n",
    "        \"\"\"Print a section header\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"  {title}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        self._write_raw({'level': 'SECTION', 'title': title})\n",
    "    \n",
    "    def log_api_call(self, method: str, params: dict):\n",
    "        \"\"\"Log a Zep-equivalent API call\"\"\"\n",
    "        self._write_raw({'level': 'API', 'method': method, 'params': params})\n",
    "        print(f\"\\n{self._indent()} ZEP API: {method}()\")\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                value = value[:100] + '...'\n",
    "            print(f\"{self._indent()}   {key}: {value}\")\n",
    "    \n",
    "    def log_graphiti_call(self, method: str, params: dict = None):\n",
    "        \"\"\"Log a graphiti_core function call\"\"\"\n",
    "        self._write_raw({'level': 'GRAPHITI', 'method': method, 'params': params})\n",
    "        print(f\"{self._indent()} GRAPHITI: {method}()\")\n",
    "        self.indent_level += 1\n",
    "    \n",
    "    def log_graphiti_step(self, step_num: int, description: str):\n",
    "        \"\"\"Log a step within a graphiti operation\"\"\"\n",
    "        self._write_raw({'level': 'GRAPHITI_STEP', 'step': step_num, 'description': description})\n",
    "        print(f\"{self._indent()}├─ Step {step_num}: {description}\")\n",
    "    \n",
    "    def log_graphiti_end(self, duration_ms: float = None, result_summary: str = None):\n",
    "        \"\"\"End a graphiti operation\"\"\"\n",
    "        self.indent_level = max(0, self.indent_level - 1)\n",
    "        if duration_ms:\n",
    "            print(f\"{self._indent()}└─ Duration: {duration_ms:.1f}ms\")\n",
    "        if result_summary:\n",
    "            print(f\"{self._indent()}   Result: {result_summary}\")\n",
    "        self._write_raw({'level': 'GRAPHITI_END', 'duration_ms': duration_ms, 'result': result_summary})\n",
    "    \n",
    "    def log_llm_call(self, purpose: str, prompt_preview: str = None):\n",
    "        \"\"\"Log an LLM call\"\"\"\n",
    "        self._write_raw({'level': 'LLM_CALL', 'purpose': purpose, 'prompt_preview': prompt_preview})\n",
    "        print(f\"{self._indent()} LLM Call: {purpose}\")\n",
    "        if prompt_preview:\n",
    "            preview = prompt_preview[:200] + '...' if len(prompt_preview) > 200 else prompt_preview\n",
    "            print(f\"{self._indent()}   Prompt: {preview}\")\n",
    "    \n",
    "    def log_llm_response(self, response_preview: str, tokens: dict = None):\n",
    "        \"\"\"Log an LLM response\"\"\"\n",
    "        self._write_raw({'level': 'LLM_RESPONSE', 'response_preview': response_preview, 'tokens': tokens})\n",
    "        preview = response_preview[:300] + '...' if len(response_preview) > 300 else response_preview\n",
    "        print(f\"{self._indent()}   Response: {preview}\")\n",
    "        if tokens:\n",
    "            print(f\"{self._indent()}   Tokens: input={tokens.get('input', '?')}, output={tokens.get('output', '?')}\")\n",
    "    \n",
    "    def log_neo4j_query(self, query: str, params: dict = None):\n",
    "        \"\"\"Log a Neo4j Cypher query\"\"\"\n",
    "        self._write_raw({'level': 'NEO4J', 'query': query, 'params': params})\n",
    "        # Clean up query for display\n",
    "        query_oneline = ' '.join(query.split())\n",
    "        if len(query_oneline) > 150:\n",
    "            query_oneline = query_oneline[:150] + '...'\n",
    "        print(f\"{self._indent()} Neo4j: {query_oneline}\")\n",
    "    \n",
    "    def log_result(self, description: str, data: any = None):\n",
    "        \"\"\"Log a result\"\"\"\n",
    "        self._write_raw({'level': 'RESULT', 'description': description, 'data': str(data) if data else None})\n",
    "        print(f\"{self._indent()} {description}\")\n",
    "        if data:\n",
    "            data_str = str(data)\n",
    "            if len(data_str) > 200:\n",
    "                data_str = data_str[:200] + '...'\n",
    "            print(f\"{self._indent()}   Data: {data_str}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the log file\"\"\"\n",
    "        self.raw_log.close()\n",
    "        print(f\"\\nTrace log saved to: {self.log_file.absolute()}\")\n",
    "\n",
    "# Initialize the trace logger\n",
    "trace_logger = TraceLogger('trace_raw.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed logging configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Configure Detailed Logging for Neo4j and HTTP\n",
    "#\n",
    "# This captures the actual queries and HTTP traffic\n",
    "\n",
    "import logging\n",
    "\n",
    "# Create a custom handler that also writes to our trace logger\n",
    "class TraceLoggingHandler(logging.Handler):\n",
    "    def __init__(self, trace_logger: TraceLogger):\n",
    "        super().__init__()\n",
    "        self.trace_logger = trace_logger\n",
    "    \n",
    "    def emit(self, record):\n",
    "        msg = self.format(record)\n",
    "        # Capture Neo4j queries\n",
    "        if 'neo4j' in record.name.lower() and 'query' in msg.lower():\n",
    "            self.trace_logger._write_raw({'level': 'NEO4J_LOG', 'message': msg})\n",
    "        # Capture HTTP requests to vLLM\n",
    "        elif 'httpx' in record.name.lower() or 'httpcore' in record.name.lower():\n",
    "            self.trace_logger._write_raw({'level': 'HTTP_LOG', 'message': msg})\n",
    "\n",
    "# Set up logging levels\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    ")\n",
    "\n",
    "# Enable DEBUG for graphiti and neo4j to see internal operations\n",
    "logging.getLogger('graphiti_core').setLevel(logging.DEBUG)\n",
    "logging.getLogger('neo4j').setLevel(logging.DEBUG)\n",
    "\n",
    "# Add our custom handler\n",
    "trace_handler = TraceLoggingHandler(trace_logger)\n",
    "logging.getLogger('graphiti_core').addHandler(trace_handler)\n",
    "logging.getLogger('neo4j').addHandler(trace_handler)\n",
    "\n",
    "print(\"Detailed logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Deep Trace Hooks (LLM + Neo4j + OTEL)\n",
    "\n",
    "This cell implements **non-invasive** hooks to capture:\n",
    "1. **LLM calls**: Full prompt and response via httpx event hooks\n",
    "2. **Neo4j queries**: Query text, parameters, and results via driver wrapper\n",
    "3. **OTEL spans**: All internal operation spans via custom SpanProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep trace hooks defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.3: Deep Trace Hooks\n",
    "#\n",
    "# These hooks capture complete information WITHOUT modifying graphiti_core source code.\n",
    "# They work by:\n",
    "# 1. Wrapping the httpx client to intercept LLM requests/responses\n",
    "# 2. Wrapping the Neo4j driver's execute_query method\n",
    "# 3. Using a custom OTEL SpanProcessor to capture all spans\n",
    "\n",
    "import httpx\n",
    "from functools import wraps\n",
    "from opentelemetry.sdk.trace import SpanProcessor\n",
    "from opentelemetry.sdk.trace import ReadableSpan\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LLM Request/Response Hook via httpx\n",
    "# ============================================================================\n",
    "\n",
    "class LLMTraceHook:\n",
    "    \"\"\"\n",
    "    Captures complete LLM request/response via httpx event hooks.\n",
    "    This is non-invasive - we just add hooks to the existing client.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceLogger):\n",
    "        self.trace = trace_logger\n",
    "        self.call_count = 0\n",
    "    \n",
    "    async def log_request(self, request: httpx.Request):\n",
    "        \"\"\"Called before each HTTP request\"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        # Only log LLM API calls (to vLLM)\n",
    "        if '/v1/chat/completions' in str(request.url):\n",
    "            try:\n",
    "                body = json.loads(request.content.decode('utf-8'))\n",
    "                messages = body.get('messages', [])\n",
    "                \n",
    "                # Log to file (full content)\n",
    "                self.trace._write_raw({\n",
    "                    'level': 'LLM_REQUEST',\n",
    "                    'call_number': self.call_count,\n",
    "                    'model': body.get('model'),\n",
    "                    'messages': messages,\n",
    "                    'temperature': body.get('temperature'),\n",
    "                    'max_tokens': body.get('max_tokens'),\n",
    "                })\n",
    "                \n",
    "                # Pretty print (truncated)\n",
    "                print(f\"\\n{self.trace._indent()}LLM Call #{self.call_count}\")\n",
    "                print(f\"{self.trace._indent()}   Model: {body.get('model')}\")\n",
    "                for i, msg in enumerate(messages):\n",
    "                    role = msg.get('role', 'unknown')\n",
    "                    content = msg.get('content', '')[:500]\n",
    "                    if len(msg.get('content', '')) > 500:\n",
    "                        content += '...'\n",
    "                    print(f\"{self.trace._indent()}   [{role}]: {content[:200]}{'...' if len(content) > 200 else ''}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM request: {e}\")\n",
    "    \n",
    "    async def log_response(self, response: httpx.Response):\n",
    "        \"\"\"Called after each HTTP response\"\"\"\n",
    "        if '/v1/chat/completions' in str(response.url):\n",
    "            try:\n",
    "                # Need to read the response body\n",
    "                await response.aread()\n",
    "                body = json.loads(response.content.decode('utf-8'))\n",
    "                \n",
    "                choices = body.get('choices', [])\n",
    "                usage = body.get('usage', {})\n",
    "                \n",
    "                content = ''\n",
    "                if choices:\n",
    "                    content = choices[0].get('message', {}).get('content', '')\n",
    "                \n",
    "                # Log to file (full content)\n",
    "                self.trace._write_raw({\n",
    "                    'level': 'LLM_RESPONSE',\n",
    "                    'call_number': self.call_count,\n",
    "                    'content': content,\n",
    "                    'usage': usage,\n",
    "                    'finish_reason': choices[0].get('finish_reason') if choices else None,\n",
    "                })\n",
    "                \n",
    "                # Pretty print (truncated)\n",
    "                content_preview = content[:300] + '...' if len(content) > 300 else content\n",
    "                print(f\"{self.trace._indent()}   Response: {content_preview}\")\n",
    "                print(f\"{self.trace._indent()}   Tokens: prompt={usage.get('prompt_tokens', '?')}, completion={usage.get('completion_tokens', '?')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM response: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Neo4j Query Hook via method wrapper\n",
    "# ============================================================================\n",
    "\n",
    "class Neo4jTraceHook:\n",
    "    \"\"\"\n",
    "    Wraps Neo4j driver's execute_query to capture queries and results.\n",
    "    Non-invasive - we just wrap the existing method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceLogger):\n",
    "        self.trace = trace_logger\n",
    "        self.query_count = 0\n",
    "    \n",
    "    def wrap_driver(self, driver):\n",
    "        \"\"\"Wrap the driver's execute_query method\"\"\"\n",
    "        original_execute = driver.execute_query\n",
    "        \n",
    "        @wraps(original_execute)\n",
    "        async def traced_execute(cypher_query, **kwargs):\n",
    "            self.query_count += 1\n",
    "            params = kwargs.get('params', {})\n",
    "            \n",
    "            # Log query start\n",
    "            self.trace._write_raw({\n",
    "                'level': 'NEO4J_QUERY',\n",
    "                'query_number': self.query_count,\n",
    "                'query': cypher_query,\n",
    "                'params': {k: str(v)[:200] for k, v in params.items()} if params else {},\n",
    "            })\n",
    "            \n",
    "            # Pretty print query\n",
    "            query_oneline = ' '.join(cypher_query.split())[:150]\n",
    "            print(f\"{self.trace._indent()} Neo4j Query #{self.query_count}: {query_oneline}{'...' if len(cypher_query) > 150 else ''}\")\n",
    "            \n",
    "            # Execute original\n",
    "            start_time = time.time()\n",
    "            result = await original_execute(cypher_query, **kwargs)\n",
    "            duration_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Log result\n",
    "            records = result.records if hasattr(result, 'records') else []\n",
    "            record_count = len(records)\n",
    "            \n",
    "            # Serialize records for logging (first 5 only)\n",
    "            records_preview = []\n",
    "            for r in records[:5]:\n",
    "                try:\n",
    "                    records_preview.append(dict(r))\n",
    "                except:\n",
    "                    records_preview.append(str(r))\n",
    "            \n",
    "            self.trace._write_raw({\n",
    "                'level': 'NEO4J_RESULT',\n",
    "                'query_number': self.query_count,\n",
    "                'duration_ms': duration_ms,\n",
    "                'record_count': record_count,\n",
    "                'records_preview': str(records_preview)[:1000],\n",
    "            })\n",
    "            \n",
    "            print(f\"{self.trace._indent()}   → {record_count} records, {duration_ms:.1f}ms\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        driver.execute_query = traced_execute\n",
    "        return driver\n",
    "\n",
    "# ============================================================================\n",
    "# 3. OTEL Span Processor for internal operations\n",
    "# ============================================================================\n",
    "\n",
    "class TraceSpanProcessor(SpanProcessor):\n",
    "    \"\"\"\n",
    "    Custom SpanProcessor that logs all spans to our trace logger.\n",
    "    This captures internal graphiti operations like entity extraction, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceLogger):\n",
    "        self.trace = trace_logger\n",
    "    \n",
    "    def on_start(self, span: ReadableSpan, parent_context=None):\n",
    "        \"\"\"Called when a span starts\"\"\"\n",
    "        span_name = span.name if hasattr(span, 'name') else str(span)\n",
    "        self.trace._write_raw({\n",
    "            'level': 'OTEL_SPAN_START',\n",
    "            'span_name': span_name,\n",
    "        })\n",
    "        # Only print for graphiti spans (not too verbose)\n",
    "        if 'graphiti' in span_name.lower() or 'zep' in span_name.lower():\n",
    "            print(f\"{self.trace._indent()}Span Start: {span_name}\")\n",
    "    \n",
    "    def on_end(self, span: ReadableSpan):\n",
    "        \"\"\"Called when a span ends\"\"\"\n",
    "        span_name = span.name\n",
    "        duration_ns = span.end_time - span.start_time if span.end_time and span.start_time else 0\n",
    "        duration_ms = duration_ns / 1_000_000\n",
    "        \n",
    "        # Get span attributes\n",
    "        attributes = dict(span.attributes) if span.attributes else {}\n",
    "        \n",
    "        self.trace._write_raw({\n",
    "            'level': 'OTEL_SPAN_END',\n",
    "            'span_name': span_name,\n",
    "            'duration_ms': duration_ms,\n",
    "            'attributes': {k: str(v) for k, v in attributes.items()},\n",
    "            'status': str(span.status) if hasattr(span, 'status') else None,\n",
    "        })\n",
    "        \n",
    "        if 'graphiti' in span_name.lower() or 'zep' in span_name.lower():\n",
    "            print(f\"{self.trace._indent()}Span End: {span_name} ({duration_ms:.1f}ms)\")\n",
    "    \n",
    "    def shutdown(self):\n",
    "        pass\n",
    "    \n",
    "    def force_flush(self, timeout_millis=None):\n",
    "        pass\n",
    "\n",
    "print(\"Deep trace hooks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j trace hook applied to driver\n",
      "LLM trace hook applied via httpx event hooks\n",
      "OTEL span processor added\n",
      "\n",
      "All deep trace hooks applied! Ready for full observability.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4: Apply Deep Trace Hooks\n",
    "#\n",
    "# This cell applies the hooks to the existing graphiti instance.\n",
    "# Run this AFTER Cell 4.5 (Graphiti initialization) and Cell 5.1 (TraceLogger).\n",
    "\n",
    "# 1. Apply Neo4j trace hook\n",
    "neo4j_hook = Neo4jTraceHook(trace_logger)\n",
    "neo4j_hook.wrap_driver(graphiti.driver)\n",
    "print(f\"Neo4j trace hook applied to driver\")\n",
    "\n",
    "# 2. Apply LLM trace hook via httpx event hooks\n",
    "# We need to access the underlying httpx client in the AsyncOpenAI client\n",
    "llm_hook = LLMTraceHook(trace_logger)\n",
    "\n",
    "# The OpenAI client uses httpx internally. We can add event hooks.\n",
    "# However, the AsyncOpenAI client doesn't expose the httpx client directly.\n",
    "# Instead, we'll create a new client with event hooks.\n",
    "\n",
    "import httpx\n",
    "\n",
    "# Create httpx client with event hooks\n",
    "traced_http_client = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(LLM_TIMEOUT_SECONDS),\n",
    "    event_hooks={\n",
    "        'request': [llm_hook.log_request],\n",
    "        'response': [llm_hook.log_response],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create new AsyncOpenAI client with traced http client\n",
    "traced_openai_client = AsyncOpenAI(\n",
    "    api_key=local_llm_api_key,\n",
    "    base_url=local_llm_base_url,\n",
    "    timeout=LLM_TIMEOUT_SECONDS,\n",
    "    http_client=traced_http_client,\n",
    ")\n",
    "\n",
    "# Update the LLM client to use the traced OpenAI client\n",
    "llm_client.client = traced_openai_client\n",
    "print(f\"LLM trace hook applied via httpx event hooks\")\n",
    "\n",
    "# 3. Add custom span processor to OTEL\n",
    "from opentelemetry import trace as otel_trace\n",
    "provider = otel_trace.get_tracer_provider()\n",
    "if hasattr(provider, 'add_span_processor'):\n",
    "    trace_span_processor = TraceSpanProcessor(trace_logger)\n",
    "    provider.add_span_processor(trace_span_processor)\n",
    "    print(f\"OTEL span processor added\")\n",
    "else:\n",
    "    print(f\"WARNING: Could not add OTEL span processor (provider type: {type(provider)})\")\n",
    "\n",
    "print(\"\\nAll deep trace hooks applied! Ready for full observability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ZepSimulator - Simulating Zep Cloud API\n",
    "\n",
    "This class mimics the Zep Python SDK interface using graphiti_core directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZepSimulator initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 6.1: ZepSimulator Class\n",
    "\n",
    "from graphiti_core.nodes import EpisodeType\n",
    "import time\n",
    "\n",
    "class ZepSimulator:\n",
    "    \"\"\"\n",
    "    Simulates Zep Cloud API behavior using graphiti_core directly.\n",
    "    \n",
    "    This provides the same interface patterns as the Zep Python SDK,\n",
    "    allowing us to see exactly what happens internally.\n",
    "    \n",
    "    Mapping:\n",
    "    - add_message() → zep_client.thread.add_messages() → graphiti.add_episode()\n",
    "    - get_user_context() → zep_client.thread.get_user_context() → graphiti.search()\n",
    "    - graph_search() → zep_client.graph.search() → graphiti.search()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, graphiti_client: Graphiti, trace_logger: TraceLogger):\n",
    "        self.graphiti = graphiti_client\n",
    "        self.trace = trace_logger\n",
    "    \n",
    "    async def add_message(\n",
    "        self,\n",
    "        group_id: str,\n",
    "        role: str,\n",
    "        name: str,\n",
    "        content: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.thread.add_messages()\n",
    "        \n",
    "        This stores a message in the knowledge graph by:\n",
    "        1. Extracting entities from the message (LLM call)\n",
    "        2. Resolving entity duplicates (LLM call)\n",
    "        3. Extracting relationships (LLM call)\n",
    "        4. Creating nodes and edges in Neo4j\n",
    "        \"\"\"\n",
    "        self.trace.log_api_call('add_message (thread.add_messages)', {\n",
    "            'group_id': group_id,\n",
    "            'role': role,\n",
    "            'name': name,\n",
    "            'content': content,\n",
    "        })\n",
    "        \n",
    "        # Format message as Zep does: \"{role}({role_type}): {content}\"\n",
    "        episode_body = f\"{name}({role}): {content}\"\n",
    "        \n",
    "        self.trace.log_graphiti_call('add_episode', {\n",
    "            'group_id': group_id,\n",
    "            'episode_body': episode_body,\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This is the actual graphiti call\n",
    "        # NOTE: Do NOT pass uuid - let graphiti auto-generate it for new episodes.\n",
    "        # If you pass a uuid, graphiti will try to fetch an existing episode with that uuid.\n",
    "        result = await self.graphiti.add_episode(\n",
    "            name=name,\n",
    "            episode_body=episode_body,\n",
    "            source_description='Agent conversation message',\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "            source=EpisodeType.message,\n",
    "            group_id=group_id,\n",
    "            # uuid is auto-generated by graphiti\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Get the uuid from the result\n",
    "        episode_uuid = result.episode.uuid\n",
    "        \n",
    "        # Log the result\n",
    "        self.trace.log_graphiti_end(\n",
    "            duration_ms=duration_ms,\n",
    "            result_summary=f\"Episode {episode_uuid[:8]}... created\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'uuid': episode_uuid,\n",
    "            'duration_ms': duration_ms,\n",
    "            'result': result,\n",
    "        }\n",
    "    \n",
    "    async def get_user_context(\n",
    "        self,\n",
    "        group_id: str,\n",
    "        query: str,\n",
    "        max_facts: int = 10,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.thread.get_user_context()\n",
    "        \n",
    "        This retrieves relevant facts from the knowledge graph by:\n",
    "        1. Generating an embedding for the query\n",
    "        2. Vector similarity search in Neo4j\n",
    "        3. Reranking results with LLM\n",
    "        4. Returning top facts\n",
    "        \"\"\"\n",
    "        self.trace.log_api_call('get_user_context (thread.get_user_context)', {\n",
    "            'group_id': group_id,\n",
    "            'query': query,\n",
    "            'max_facts': max_facts,\n",
    "        })\n",
    "        \n",
    "        self.trace.log_graphiti_call('search', {\n",
    "            'group_ids': [group_id],\n",
    "            'query': query,\n",
    "            'num_results': max_facts,\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This is the actual graphiti call\n",
    "        results = await self.graphiti.search(\n",
    "            group_ids=[group_id],\n",
    "            query=query,\n",
    "            num_results=max_facts,\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Extract facts from results\n",
    "        facts = [edge.fact for edge in results]\n",
    "        \n",
    "        self.trace.log_graphiti_end(\n",
    "            duration_ms=duration_ms,\n",
    "            result_summary=f\"Found {len(facts)} facts\"\n",
    "        )\n",
    "        \n",
    "        # Log each fact\n",
    "        for i, fact in enumerate(facts):\n",
    "            self.trace.log_result(f\"Fact {i+1}\", fact)\n",
    "        \n",
    "        return facts\n",
    "    \n",
    "    async def graph_search(\n",
    "        self,\n",
    "        group_ids: list[str],\n",
    "        query: str,\n",
    "        max_facts: int = 10,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Equivalent to: zep_client.graph.search()\n",
    "        \n",
    "        Same as get_user_context but can search across multiple groups.\n",
    "        \"\"\"\n",
    "        self.trace.log_api_call('graph_search (graph.search)', {\n",
    "            'group_ids': group_ids,\n",
    "            'query': query,\n",
    "            'max_facts': max_facts,\n",
    "        })\n",
    "        \n",
    "        self.trace.log_graphiti_call('search', {\n",
    "            'group_ids': group_ids,\n",
    "            'query': query,\n",
    "            'num_results': max_facts,\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = await self.graphiti.search(\n",
    "            group_ids=group_ids,\n",
    "            query=query,\n",
    "            num_results=max_facts,\n",
    "        )\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        self.trace.log_graphiti_end(\n",
    "            duration_ms=duration_ms,\n",
    "            result_summary=f\"Found {len(results)} edges\"\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the simulator\n",
    "zep = ZepSimulator(graphiti, trace_logger)\n",
    "print(\"ZepSimulator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent Conversation Demo\n",
    "\n",
    "This demonstrates a real-world conversation that shows:\n",
    "- **Adding nodes**: User introduces themselves\n",
    "- **Adding relationships**: User mentions their work\n",
    "- **Updating**: User provides more information\n",
    "- **Searching**: Agent retrieves context\n",
    "\n",
    "The demo simulates agent_memory_full_example pattern:\n",
    "\n",
    "- Turn 1: User introduces themselves → Creates Alice Chen, TechCorp entities\n",
    "- Turn 2: User mentions project → Creates Project Phoenix, LEADS relationship\n",
    "- Turn 3: User provides deadline → Updates with deadline info\n",
    "- Turn 4: Search test → Retrieves context about Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Group ID: demo_session_20260131_213508\n",
      "User: Alice Chen\n",
      "Conversation turns: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.1: Define the Conversation\n",
    "\n",
    "# Use a unique group_id for this demo session\n",
    "GROUP_ID = f\"demo_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "USER_NAME = \"Alice Chen\"\n",
    "\n",
    "# The conversation turns\n",
    "CONVERSATION = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"description\": \"User introduces themselves - creates new entities\",\n",
    "        \"user_message\": \"Hi, I'm Alice Chen. I work at TechCorp as a senior software engineer.\",\n",
    "        \"expected_entities\": [\"Alice Chen (Person)\", \"TechCorp (Organization)\"],\n",
    "        \"expected_relationships\": [\"Alice Chen WORKS_AT TechCorp\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"description\": \"User mentions a project - adds more entities\",\n",
    "        \"user_message\": \"I'm currently leading Project Phoenix, which is a cloud migration initiative.\",\n",
    "        \"expected_entities\": [\"Project Phoenix (Project)\"],\n",
    "        \"expected_relationships\": [\"Alice Chen LEADS Project Phoenix\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 3,\n",
    "        \"description\": \"User provides deadline - updates existing entity\",\n",
    "        \"user_message\": \"The project deadline is February 15th, and we have 3 team members.\",\n",
    "        \"expected_entities\": [],\n",
    "        \"expected_relationships\": [\"Project Phoenix HAS_DEADLINE February 15th\"],\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 4,\n",
    "        \"description\": \"Search test - retrieve context about Alice\",\n",
    "        \"search_query\": \"What does Alice work on?\",\n",
    "        \"expected_facts\": [\"Alice works at TechCorp\", \"Alice leads Project Phoenix\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Demo Group ID: {GROUP_ID}\")\n",
    "print(f\"User: {USER_NAME}\")\n",
    "print(f\"Conversation turns: {len(CONVERSATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Run Turn 1 - User Introduction\n",
    "\n",
    "turn = CONVERSATION[0]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected entities: {turn['expected_entities']}\")\n",
    "print(f\"Expected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Add the user message\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 1 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.3: Run Turn 2 - Project Information\n",
    "\n",
    "turn = CONVERSATION[1]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected entities: {turn['expected_entities']}\")\n",
    "print(f\"Expected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 2 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.4: Run Turn 3 - Deadline Update\n",
    "\n",
    "turn = CONVERSATION[2]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nUser: {turn['user_message']}\")\n",
    "print(f\"\\nExpected relationships: {turn['expected_relationships']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "result = await zep.add_message(\n",
    "    group_id=GROUP_ID,\n",
    "    role=\"user\",\n",
    "    name=USER_NAME,\n",
    "    content=turn['user_message'],\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 3 complete. Duration: {result['duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.5: Run Turn 4 - Search Test\n",
    "\n",
    "turn = CONVERSATION[3]\n",
    "trace_logger.log_section(f\"TURN {turn['turn']}: {turn['description']}\")\n",
    "\n",
    "print(f\"\\nSearch Query: {turn['search_query']}\")\n",
    "print(f\"\\nExpected facts: {turn['expected_facts']}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Search for context\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=turn['search_query'],\n",
    "    max_facts=10,\n",
    ")\n",
    "\n",
    "print(f\"\\nOK: Turn 4 complete. Found {len(facts)} facts.\")\n",
    "print(\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   TURN 4: Search test - retrieve context about Alice\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: What does Alice work on?\n",
    "\n",
    "# Expected facts: ['Alice works at TechCorp', 'Alice leads Project Phoenix']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: What does Alice work on?\n",
    "#    max_facts: 10\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #36: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #37: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "#       → 3 records, 67.1ms\n",
    "#       → 2 records, 60.2ms\n",
    "# └─ Duration: 93.2ms\n",
    "#    Result: Found 3 facts\n",
    "#  Fact 1\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "#  Fact 2\n",
    "# ...\n",
    "# Retrieved Facts:\n",
    "#    1. Alice Chen is currently leading Project Phoenix.\n",
    "#    2. Project Phoenix has a deadline on February 15th.\n",
    "#    3. Alice Chen works at TechCorp as a senior software engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional Search Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.1: Search for project deadline\n",
    "\n",
    "trace_logger.log_section(\"ADDITIONAL SEARCH: Project Deadline\")\n",
    "\n",
    "query = \"When is the project deadline?\"\n",
    "print(f\"\\nSearch Query: {query}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   ADDITIONAL SEARCH: Project Deadline\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: When is the project deadline?\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: When is the project deadline?\n",
    "#    max_facts: 5\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #38: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #39: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: RECORD * 1\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: SUCCESS {'statuses': [{'gql_status': '00000', 'status_description': 'note: successful completion'}], 'type': 'r', 't_last': 1, 'db': 'neo4j'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  C: COMMIT\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  _: <CONNECTION> client state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E86E]  S: SUCCESS {'bookmark': 'FB:kcwQLskJJyV+REC9lj1ew4ZBjkqQ'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E86E]  _: <CONNECTION> server state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.pool | DEBUG | [#E86E]  _: <POOL> released bolt-104875\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  S: SUCCESS {'bookmark': 'FB:kcwQLskJJyV+REC9lj1ew4ZBjkqQ'}\n",
    "# 21:35:27 | neo4j.io | DEBUG | [#E860]  _: <CONNECTION> server state: TX_READY_OR_TX_STREAMING > READY\n",
    "# 21:35:27 | neo4j.pool | DEBUG | [#E860]  _: <POOL> released bolt-104890\n",
    "# 21:35:27 | graphiti_core.search.search | DEBUG | search returned context for query When is the project deadline? in 89.6604061126709 ms\n",
    "#       → 2 records, 64.8ms\n",
    "#       → 2 records, 55.6ms\n",
    "# └─ Duration: 91.1ms\n",
    "#    Result: Found 2 facts\n",
    "#  Fact 1\n",
    "#    Data: Project Phoenix has a deadline on February 15th.\n",
    "#  Fact 2\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "\n",
    "# Retrieved Facts:\n",
    "#    1. Project Phoenix has a deadline on February 15th.\n",
    "#    2. Alice Chen is currently leading Project Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.2: Search for company information\n",
    "\n",
    "trace_logger.log_section(\"ADDITIONAL SEARCH: Company Information\")\n",
    "\n",
    "query = \"What company does Alice work for?\"\n",
    "print(f\"\\nSearch Query: {query}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "facts = await zep.get_user_context(\n",
    "    group_id=GROUP_ID,\n",
    "    query=query,\n",
    "    max_facts=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nRetrieved Facts:\")\n",
    "for i, fact in enumerate(facts):\n",
    "    print(f\"   {i+1}. {fact}\")\n",
    "\n",
    "# demo output\n",
    "# ======================================================================\n",
    "#   ADDITIONAL SEARCH: Company Information\n",
    "# ======================================================================\n",
    "\n",
    "# Search Query: What company does Alice work for?\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "#  ZEP API: get_user_context (thread.get_user_context)()\n",
    "#    group_id: demo_session_20260131_213508\n",
    "#    query: What company does Alice work for?\n",
    "#    max_facts: 5\n",
    "#  GRAPHITI: search()\n",
    "#     Neo4j Query #40: CALL db.index.fulltext.queryRelationships(\"edge_name_and_fact\", $query, {limit: $limit}) YIELD relationship AS rel, score MATCH (n:Entity)-[e:RELATES_...\n",
    "#     Neo4j Query #41: MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity) WHERE e.group_id IN $group_ids WITH DISTINCT e, n, m, vector.similarity.cosine(e.fact_embedding, $search_v...\n",
    "#       → 2 records, 70.1ms\n",
    "#       → 3 records, 60.5ms\n",
    "# 21:35:27 | graphiti_core.search.search | DEBUG | search returned context for query What company does Alice work for? in 95.98636627197266 ms\n",
    "# └─ Duration: 97.9ms\n",
    "#    Result: Found 3 facts\n",
    "#  Fact 1\n",
    "#    Data: Alice Chen is currently leading Project Phoenix.\n",
    "#  Fact 2\n",
    "#    Data: Alice Chen works at TechCorp as a senior software engineer.\n",
    "#  Fact 3\n",
    "#    Data: Project Phoenix has a deadline on February 15th.\n",
    "\n",
    "# Retrieved Facts:\n",
    "#    1. Alice Chen is currently leading Project Phoenix.\n",
    "#    2. Alice Chen works at TechCorp as a senior software engineer.\n",
    "#    3. Project Phoenix has a deadline on February 15th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  DEMO COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Trace log saved to: /mnt/data-disk-1/home/cpii.local/ericlo/projects/zep-repos/zep-graphiti/examples/neo4j_otel/trace_raw.jsonl\n",
      "\n",
      "======================================================================\n",
      "  SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Raw trace log saved to: trace_raw.jsonl\n",
      "   - Contains all API calls, LLM calls, and Neo4j queries in JSON format\n",
      "   - Use for debugging and detailed analysis\n",
      "\n",
      "Demo Group ID: demo_session_20260131_213508\n",
      "   - Use this to query the graph directly in Neo4j Browser\n",
      "\n",
      "Neo4j Browser Query:\n",
      "   MATCH (n) WHERE n.group_id = 'demo_session_20260131_213508' RETURN n\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.1: Close trace logger and show summary\n",
    "\n",
    "trace_logger.log_section(\"DEMO COMPLETE\")\n",
    "trace_logger.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRaw trace log saved to: trace_raw.jsonl\")\n",
    "print(f\"   - Contains all API calls, LLM calls, and Neo4j queries in JSON format\")\n",
    "print(f\"   - Use for debugging and detailed analysis\")\n",
    "print(f\"\\nDemo Group ID: {GROUP_ID}\")\n",
    "print(f\"   - Use this to query the graph directly in Neo4j Browser\")\n",
    "print(f\"\\nNeo4j Browser Query:\")\n",
    "print(f\"   MATCH (n) WHERE n.group_id = '{GROUP_ID}' RETURN n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lines of trace_raw.jsonl:\n",
      "\n",
      "  1. [SECTION        ] {\"level\": \"SECTION\", \"title\": \"TURN 1: User introduces themselves - creates new entities\", \"timestam...\n",
      "  2. [API            ] {\"level\": \"API\", \"method\": \"add_message (thread.add_messages)\", \"params\": {\"group_id\": \"demo_session...\n",
      "  3. [GRAPHITI       ] {\"level\": \"GRAPHITI\", \"method\": \"add_episode\", \"params\": {\"group_id\": \"demo_session_20260131_213508\"...\n",
      "  4. [OTEL_SPAN_START] {\"level\": \"OTEL_SPAN_START\", \"span_name\": \"zep.graphiti.add_episode\", \"timestamp\": \"2026-01-31T13:35...\n",
      "  5. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 1, \"query\": \"\\n                                    MATCH (e...\n",
      "  6. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 1, \"duration_ms\": 30.347824096679688, \"record_count\": 0, \"...\n",
      "  7. [OTEL_SPAN_START] {\"level\": \"OTEL_SPAN_START\", \"span_name\": \"zep.graphiti.llm.generate\", \"timestamp\": \"2026-01-31T13:3...\n",
      "  8. [LLM_REQUEST    ] {\"level\": \"LLM_REQUEST\", \"call_number\": 1, \"model\": \"Qwen/Qwen2.5-32B-Instruct\", \"messages\": [{\"role...\n",
      "  9. [LLM_RESPONSE   ] {\"level\": \"LLM_RESPONSE\", \"call_number\": 1, \"content\": \"{\\n  \\\"extracted_entities\\\": [\\n    {\\n     ...\n",
      " 10. [OTEL_SPAN_END  ] {\"level\": \"OTEL_SPAN_END\", \"span_name\": \"zep.graphiti.llm.generate\", \"duration_ms\": 1671.015729, \"at...\n",
      " 11. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 2, \"query\": \"CALL db.index.fulltext.queryNodes(\\\"node_name_...\n",
      " 12. [NEO4J_LOG      ] {\"level\": \"NEO4J_LOG\", \"message\": \"[#E86E]  C: RUN 'CALL db.index.fulltext.queryNodes(\\\"node_name_an...\n",
      " 13. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 3, \"query\": \"\\n                                            ...\n",
      " 14. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 4, \"query\": \"CALL db.index.fulltext.queryNodes(\\\"node_name_...\n",
      " 15. [NEO4J_LOG      ] {\"level\": \"NEO4J_LOG\", \"message\": \"[#E8C4]  C: RUN 'CALL db.index.fulltext.queryNodes(\\\"node_name_an...\n",
      " 16. [NEO4J_QUERY    ] {\"level\": \"NEO4J_QUERY\", \"query_number\": 5, \"query\": \"\\n                                            ...\n",
      " 17. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 84.91873741149902, \"record_count\": 0, \"r...\n",
      " 18. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 86.10320091247559, \"record_count\": 0, \"r...\n",
      " 19. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 63.6904239654541, \"record_count\": 0, \"re...\n",
      " 20. [NEO4J_RESULT   ] {\"level\": \"NEO4J_RESULT\", \"query_number\": 5, \"duration_ms\": 50.745248794555664, \"record_count\": 0, \"...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.2: View the raw trace log\n",
    "\n",
    "print(\"First 20 lines of trace_raw.jsonl:\\n\")\n",
    "with open('trace_raw.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 20:\n",
    "            print(\"...\")\n",
    "            break\n",
    "        entry = json.loads(line)\n",
    "        print(f\"{i+1:3d}. [{entry.get('level', 'UNKNOWN'):15s}] {json.dumps(entry)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.3: Close Graphiti connection\n",
    "\n",
    "await graphiti.close()\n",
    "print(\"Graphiti connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete Execution Flow Analysis\n",
    "\n",
    "This section documents the complete execution flow of the demo, based on actual trace logs captured during execution.\n",
    "\n",
    "### 10.1 Execution Summary\n",
    "\n",
    "| Turn | Description | Duration | LLM Calls | Neo4j Queries | Entities Created | Relationships Created |\n",
    "|------|-------------|----------|-----------|---------------|------------------|----------------------|\n",
    "| 1 | User Introduction | 6813ms | 5 | 10 | Alice Chen, TechCorp | WORKS_AT |\n",
    "| 2 | Project Information | 6273ms | 6 | 10 | Project Phoenix | LEADS_PROJECT |\n",
    "| 3 | Deadline Update | 5881ms | 6 | 15 | - | PROJECT_DEADLINE |\n",
    "| 4 | Search Test | ~130ms | 0 | 2 | - | - |\n",
    "\n",
    "**Total Log Entries:** 206  \n",
    "**Log Level Distribution:**\n",
    "- LLM_REQUEST/RESPONSE: 17 each\n",
    "- NEO4J_QUERY/RESULT: 41 each  \n",
    "- OTEL_SPAN_START/END: 20 each\n",
    "- API: 6, GRAPHITI: 6, SECTION: 7\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Turn 1: User Introduction (6813ms)\n",
    "\n",
    "**Input Message:**\n",
    "```\n",
    "Alice Chen(user): Hi, I'm Alice Chen. I work at TechCorp as a senior software engineer.\n",
    "```\n",
    "\n",
    "**Execution Flow:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 0: Query Previous Episodes                                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Neo4j Query #1 (30.3ms):                                                    │\n",
    "│   MATCH (e:Episodic) WHERE e.valid_at <= $reference_time                    │\n",
    "│   AND e.group_id IN $group_ids AND e.source = $source                       │\n",
    "│   RETURN e ORDER BY e.valid_at DESC LIMIT $num_episodes                     │\n",
    "│ Result: 0 records (first message, no history)                               │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 1: Extract Entities (LLM Call #1) - 1671ms                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Prompt: extract_nodes.extract_message                                       │\n",
    "│ Tokens: prompt=491, completion=54, total=545                                │\n",
    "│                                                                             │\n",
    "│ System: \"You are an AI assistant that extracts entity nodes from            │\n",
    "│          conversational messages...\"                                        │\n",
    "│                                                                             │\n",
    "│ User Input:                                                                 │\n",
    "│   <ENTITY TYPES>                                                            │\n",
    "│   [{'entity_type_id': 0, 'entity_type_name': 'Entity',                      │\n",
    "│     'entity_type_description': 'Default entity classification...'}]         │\n",
    "│   </ENTITY TYPES>                                                           │\n",
    "│   <PREVIOUS MESSAGES>[]</PREVIOUS MESSAGES>                                 │\n",
    "│   <CURRENT MESSAGE>                                                         │\n",
    "│   Alice Chen(user): Hi, I'm Alice Chen. I work at TechCorp...               │\n",
    "│   </CURRENT MESSAGE>                                                        │\n",
    "│                                                                             │\n",
    "│ Response:                                                                   │\n",
    "│   {                                                                         │\n",
    "│     \"extracted_entities\": [                                                 │\n",
    "│       {\"name\": \"Alice Chen\", \"entity_type_id\": 0},                          │\n",
    "│       {\"name\": \"TechCorp\", \"entity_type_id\": 0}                             │\n",
    "│     ]                                                                       │\n",
    "│   }                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 2: Search Existing Entities (Neo4j Parallel Queries) - ~285ms          │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ For each extracted entity (Alice Chen, TechCorp), run parallel searches:    │\n",
    "│                                                                             │\n",
    "│ Query #2 - BM25 Fulltext (TechCorp):                                        │\n",
    "│   CALL db.index.fulltext.queryNodes(\\\"node_name_and_summary\\\", $query, ...) │\n",
    "│   Result: 0 records                                                         │\n",
    "│                                                                             │\n",
    "│ Query #3 - Cosine Similarity (TechCorp):                                    │\n",
    "│   MATCH (n:Entity) WHERE n.group_id IN $group_ids                           │\n",
    "│   WITH n, vector.similarity.cosine(n.name_embedding,$search_vector) AS score│\n",
    "│   Result: 0 records                                                         │\n",
    "│                                                                             │\n",
    "│ Query #4 - BM25 Fulltext (Alice Chen):                                      │\n",
    "│   Result: 0 records                                                         │\n",
    "│                                                                             │\n",
    "│ Query #5 - Cosine Similarity (Alice Chen):                                  │\n",
    "│   Result: 0 records                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 3: Deduplicate Entities (LLM Call #2) - 1278ms                         │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Prompt: dedupe_nodes.nodes                                                  │\n",
    "│ Tokens: prompt=663, completion=78, total=741                                │\n",
    "│                                                                             │\n",
    "│ System: \"You are a helpful assistant that determines whether or not         │\n",
    "│          ENTITIES extracted from a conversation are duplicates...\"          │\n",
    "│                                                                             │\n",
    "│ User Input:                                                                 │\n",
    "│   <ENTITIES>                                                                │\n",
    "│   [{\"id\": 0, \"name\": \"Alice Chen\", \"entity_type\": [\"Entity\"]},              │\n",
    "│    {\"id\": 1, \"name\": \"TechCorp\", \"entity_type\": [\"Entity\"]}]                │\n",
    "│   </ENTITIES>                                                               │\n",
    "│   <EXISTING ENTITIES>[]</EXISTING ENTITIES>                                 │\n",
    "│                                                                             │\n",
    "│ Response:                                                                   │\n",
    "│   {                                                                         │\n",
    "│     \"entity_resolutions\": [                                                 │\n",
    "│       {\"id\": 0, \"name\": \"Alice Chen\",\"duplicate_idx\": -1, \"duplicates\": []},│\n",
    "│       {\"id\": 1, \"name\": \"TechCorp\", \"duplicate_idx\": -1, \"duplicates\": []}  │\n",
    "│     ]                                                                       │\n",
    "│   }                                                                         │\n",
    "│ → Both entities are NEW (no duplicates found)                               │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 4: Extract Relationships (LLM Call #3) - 2249ms                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Prompt: extract_edges.edge                                                  │\n",
    "│ Tokens: prompt=774, completion=100, total=874                               │\n",
    "│                                                                             │\n",
    "│ System: \"You are an expert fact extractor that extracts fact triples...\"    │\n",
    "│                                                                             │\n",
    "│ User Input:                                                                 │\n",
    "│   <ENTITIES>                                                                │\n",
    "│   [{\"id\": 0, \"name\": \"Alice Chen\", \"entity_types\": [\"Entity\"]},             │\n",
    "│    {\"id\": 1, \"name\": \"TechCorp\", \"entity_types\": [\"Entity\"]}]               │\n",
    "│   </ENTITIES>                                                               │\n",
    "│   <CURRENT_MESSAGE>Alice Chen(user): Hi, I'm Alice Chen...</CURRENT_MESSAGE>│\n",
    "│   <REFERENCE_TIME>2026-01-31 13:35:08.650972+00:00</REFERENCE_TIME>         │\n",
    "│                                                                             │\n",
    "│ Response:                                                                   │\n",
    "│   {                                                                         │\n",
    "│     \"edges\": [{                                                             │\n",
    "│       \"relation_type\": \"WORKS_AT\",                                          │\n",
    "│       \"source_entity_id\": 0,                                                │\n",
    "│       \"target_entity_id\": 1,                                                │\n",
    "│       \"fact\": \"Alice Chen works at TechCorp as a senior software engineer.\",│\n",
    "│       \"valid_at\": \"2026-01-31T13:35:08Z\",                                   │\n",
    "│       \"invalid_at\": null                                                    │\n",
    "│     }]                                                                      │\n",
    "│   }                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 5: Search Existing Edges (Neo4j Queries) - ~418ms                      │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Query #6 - Direct Edge Lookup (27ms):                                       │\n",
    "│   MATCH (n:Entity {uuid: $source})-[e:RELATES_TO]->(m:Entity {uuid: $target})│\n",
    "│   Result: 0 records                                                         │\n",
    "│                                                                             │\n",
    "│ Query #7-8 - BM25 + Cosine on existing edges:                               │\n",
    "│   CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", ...)     │\n",
    "│   Result: 0 records (no existing edges to compare)                          │\n",
    "│                                                                             │\n",
    "│ Query #9-10 - Additional edge searches:                                     │\n",
    "│   Result: 0 records                                                         │\n",
    "│                                                                             │\n",
    "│ → No existing edges found, skip deduplication LLM call                      │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 6: Generate Entity Summaries (LLM Calls #4-5 PARALLEL) - ~566ms        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Prompt: extract_nodes.extract_summary (run in parallel for each entity)     │\n",
    "│                                                                             │\n",
    "│ LLM Call #4 - TechCorp Summary (534ms):                                     │\n",
    "│   Tokens: prompt=425, completion=17, total=442                              │\n",
    "│   Response: {\"summary\": \"TechCorp employs Alice Chen as a senior software   │\n",
    "│              engineer.\"}                                                    │\n",
    "│                                                                             │\n",
    "│ LLM Call #5 - Alice Chen Summary (566ms):                                   │\n",
    "│   Tokens: prompt=425, completion=18, total=443                              │\n",
    "│   Response: {\"summary\": \"Alice Chen works at TechCorp as a senior software  │\n",
    "│              engineer.\"}                                                    │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ RESULT: Episode Created                                                     │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Episode UUID: 03013a24-a05b-4ed8-b57a-28dad9073f35                          │\n",
    "│ Total Duration: 6808.7ms                                                    │\n",
    "│                                                                             │\n",
    "│ Created:                                                                    │\n",
    "│   - Entity: Alice Chen (uuid: 9fbefe48-8540-4878-ac2a-1f6d0e89c3e8)         │\n",
    "│   - Entity: TechCorp (uuid: 10633107-3301-4b90-a8d3-96375555b786)           │\n",
    "│   - Edge: Alice Chen --[WORKS_AT]--> TechCorp                               │\n",
    "│   - Episodic: Original message stored                                       │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Turn 2: Project Information (6273ms)\n",
    "\n",
    "**Input Message:**\n",
    "```\n",
    "Alice Chen(user): I'm currently leading Project Phoenix, which is a cloud migration initiative.\n",
    "```\n",
    "\n",
    "**Key Differences from Turn 1:**\n",
    "\n",
    "1. **Entity Extraction (LLM #6):** Extracts `Alice Chen` and `Project Phoenix`\n",
    "\n",
    "2. **Entity Deduplication (LLM #7):**\n",
    "   - `Alice Chen` → Found existing entity (idx: 0), marked as duplicate\n",
    "   - `Project Phoenix` → New entity (duplicate_idx: -1)\n",
    "\n",
    "3. **Relationship Extraction (LLM #8):**\n",
    "   ```json\n",
    "   {\n",
    "     \"edges\": [{\n",
    "       \"relation_type\": \"LEADS_PROJECT\",\n",
    "       \"source_entity_id\": 0,\n",
    "       \"target_entity_id\": 1,\n",
    "       \"fact\": \"Alice Chen is currently leading Project Phoenix.\",\n",
    "       \"valid_at\": \"2026-01-31T13:35:15Z\"\n",
    "     }]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Edge Deduplication (LLM #9):**\n",
    "   - Checks against existing edge: `Alice Chen WORKS_AT TechCorp`\n",
    "   - Result: No duplicates, no contradictions\n",
    "\n",
    "5. **Summary Updates (LLM #10-11):**\n",
    "   - Project Phoenix: `\"Cloud migration initiative led by Alice Chen at TechCorp.\"`\n",
    "   - Alice Chen: Updated to include project leadership\n",
    "\n",
    "**Created:**\n",
    "- Entity: Project Phoenix\n",
    "- Edge: Alice Chen --[LEADS_PROJECT]--> Project Phoenix\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Turn 3: Deadline Update (5881ms)\n",
    "\n",
    "**Input Message:**\n",
    "```\n",
    "Alice Chen(user): The deadline for Project Phoenix is February 15th. We're almost ready to go live.\n",
    "```\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Entity Extraction (LLM #12):** Same entities (Alice Chen, Project Phoenix)\n",
    "\n",
    "2. **No Entity Deduplication LLM Call:** Both entities already exist\n",
    "\n",
    "3. **Relationship Extraction (LLM #13):** Extracts TWO relationships:\n",
    "   ```json\n",
    "   {\n",
    "     \"edges\": [\n",
    "       {\n",
    "         \"relation_type\": \"LEADS_PROJECT\",\n",
    "         \"fact\": \"Alice Chen is leading Project Phoenix.\"\n",
    "       },\n",
    "       {\n",
    "         \"relation_type\": \"PROJECT_DEADLINE\",\n",
    "         \"fact\": \"Project Phoenix has a deadline on February 15th.\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Edge Deduplication (LLM #14-15):**\n",
    "   - LEADS_PROJECT: `duplicate_facts: [0]` → Duplicate of existing edge, SKIPPED\n",
    "   - PROJECT_DEADLINE: New fact, CREATED\n",
    "\n",
    "5. **More Neo4j Queries (15 vs 10):** Additional queries to find existing edges for deduplication\n",
    "\n",
    "**Created:**\n",
    "- Edge: Project Phoenix --[PROJECT_DEADLINE]--> (with fact about Feb 15th)\n",
    "\n",
    "---\n",
    "\n",
    "### 10.5 Turn 4: Search Test (~130ms)\n",
    "\n",
    "**Query:**\n",
    "```\n",
    "What does Alice work on?\n",
    "```\n",
    "\n",
    "**Execution Flow:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 1: Generate Query Embedding (Local)                                    │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Embedder: sentence-transformers/all-MiniLM-L6-v2                            │\n",
    "│ Input: \"What does Alice work on?\"                                           │\n",
    "│ Output: 384-dimensional vector                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 2: Execute Parallel Searches                                           │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Query #36 - BM25 Fulltext Search (67.1ms):                                  │\n",
    "│   CALL db.index.fulltext.queryRelationships(\\\"edge_name_and_fact\\\", $query) │\n",
    "│   YIELD relationship AS rel, score                                          │\n",
    "│   MATCH (n:Entity)-[e:RELATES_TO {uuid: rel.uuid}]->(m:Entity)              │\n",
    "│   WHERE e.group_id IN $group_ids                                            │\n",
    "│   Result: 3 records                                                         │\n",
    "│                                                                             │\n",
    "│ Query #37 - Cosine Similarity Search (60.2ms):                              │\n",
    "│   MATCH (n:Entity)-[e:RELATES_TO]->(m:Entity)                               │\n",
    "│   WHERE e.group_id IN $group_ids                                            │\n",
    "│   WITH DISTINCT e, n, m,                                                    │\n",
    "│        vector.similarity.cosine(e.fact_embedding, $search_vector) AS score  │\n",
    "│   WHERE score > $min_score                                                  │\n",
    "│   Result: 2 records                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ STEP 3: Merge and Return Results                                            │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ Total Duration: 93.2ms                                                      │\n",
    "│                                                                             │\n",
    "│ Retrieved Facts (3):                                                        │\n",
    "│   1. \"Alice Chen is currently leading Project Phoenix.\"                     │\n",
    "│   2. \"Project Phoenix has a deadline on February 15th.\"                     │\n",
    "│   3. \"Alice Chen works at TechCorp as a senior software engineer.\"          │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Observation:** Search is extremely fast (~130ms) because:\n",
    "- No LLM calls required\n",
    "- Only 2 Neo4j queries (parallel)\n",
    "- Results are pre-indexed with embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### 10.6 Final Knowledge Graph State\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────┐\n",
    "                    │      TechCorp       │\n",
    "                    │     (Entity)        │\n",
    "                    │                     │\n",
    "                    │ Summary: \"TechCorp  │\n",
    "                    │ employs Alice Chen  │\n",
    "                    │ as a senior software│\n",
    "                    │ engineer.\"          │\n",
    "                    └──────────▲──────────┘\n",
    "                               │\n",
    "                               │ WORKS_AT\n",
    "                               │ \"Alice Chen works at TechCorp\n",
    "                               │  as a senior software engineer.\"\n",
    "                               │\n",
    "┌──────────────────────────────┴──────────────────────────────┐\n",
    "│                        Alice Chen                           │\n",
    "│                         (Entity)                            │\n",
    "│                                                             │\n",
    "│ Summary: \"Alice Chen works at TechCorp as a senior software │\n",
    "│          engineer, leading Project Phoenix, a cloud         │\n",
    "│          migration initiative with a Feb 15th deadline.\"    │\n",
    "└──────────────────────────────┬──────────────────────────────┘\n",
    "                               │\n",
    "                               │ LEADS_PROJECT\n",
    "                               │ \"Alice Chen is currently\n",
    "                               │  leading Project Phoenix.\"\n",
    "                               │\n",
    "                               ▼\n",
    "                    ┌─────────────────────┐\n",
    "                    │   Project Phoenix   │\n",
    "                    │      (Entity)       │\n",
    "                    │                     │\n",
    "                    │ Summary: \"Cloud     │\n",
    "                    │ migration initiative│\n",
    "                    │ led by Alice Chen.  │\n",
    "                    │ Deadline: Feb 15th.\"│\n",
    "                    └──────────┬──────────┘\n",
    "                               │\n",
    "                               │ PROJECT_DEADLINE\n",
    "                               │ \"Project Phoenix has a\n",
    "                               │  deadline on February 15th.\"\n",
    "                               │\n",
    "                               ▼\n",
    "                         (implicit target)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10.7 Performance Analysis\n",
    "\n",
    "**Time Breakdown by Component:**\n",
    "\n",
    "| Component | Turn 1 | Turn 2 | Turn 3 | Turn 4 |\n",
    "|-----------|--------|--------|--------|--------|\n",
    "| LLM Calls | ~6106ms (89.6%) | ~6009ms (95.8%) | ~4835ms (82.2%) | 0ms |\n",
    "| Neo4j Queries | ~704ms (10.3%) | ~264ms (4.2%) | ~1047ms (17.8%) | ~127ms (100%) |\n",
    "| Other | ~3ms (0.1%) | 0ms | 0ms | ~3ms |\n",
    "| **Total** | **6813ms** | **6273ms** | **5881ms** | **~130ms** |\n",
    "\n",
    "another angle: read is db performance, but write is not\n",
    "\n",
    "| Metric | Write (add_episode) | Read (search) |\n",
    "|--------|---------------------|---------------|\n",
    "| **Primary Bottleneck** | LLM (82-96% of time) | DB + Embedding |\n",
    "| **Neo4j Time** | ~264-1047ms (4-18%) | ~127ms (100%) |\n",
    "| **LLM Time** | ~4835-6106ms | 0ms |\n",
    "| **Limiting Factor** | LLM API concurrency | Neo4j indexing efficiency |\n",
    "\n",
    "**LLM Call Breakdown (Turn 1):**\n",
    "\n",
    "| Step | Prompt Name | Duration | Tokens |\n",
    "|------|-------------|----------|--------|\n",
    "| Extract Entities | extract_nodes.extract_message | 1671ms | 545 |\n",
    "| Dedupe Entities | dedupe_nodes.nodes | 1278ms | 741 |\n",
    "| Extract Edges | extract_edges.edge | 2249ms | 874 |\n",
    "| Summary (TechCorp) | extract_nodes.extract_summary | 534ms | 442 |\n",
    "| Summary (Alice) | extract_nodes.extract_summary | 566ms | 443 |\n",
    "\n",
    "**Observations:**\n",
    "1. **LLM is the bottleneck:** 82-96% of execution time is spent on LLM calls\n",
    "2. **Edge extraction is slowest:** ~2.2s for relationship extraction\n",
    "3. **Parallel execution helps:** Summary generation runs in parallel\n",
    "4. **Search is fast:** No LLM needed, only vector similarity search\n",
    "5. **Neo4j is efficient:** ofc\n",
    "\n",
    "---\n",
    "\n",
    "### 10.8 Neo4j's Role\n",
    "\n",
    "To clarify Neo4j's actual role in this architecture:\n",
    "\n",
    "perhaps **Neo4j is NOT a \\\"Graph Operator with LLM\\\"** - it is just a storage engine with vector retrieval capabilities, so zep is kinada like other agent mem papers but a bit better: still a **AI Agent Orchestration Framework Built on Top of a Database**.\n",
    "\n",
    "| Aspect | Neo4j's Role | NOT Neo4j's Role |\n",
    "|--------|--------------|------------------|\n",
    "| Storage | ✅ Store nodes, edges, embeddings | |\n",
    "| Query | ✅ Execute Cypher, BM25, vector similarity | |\n",
    "| Reasoning | | ❌ Handled by LLM in Python layer |\n",
    "| Entity Resolution | | ❌ \\\"Alice\\\" vs \\\"Alice Chen\\\" decided by LLM |\n",
    "| Graph Integrity | | ❌ Deduplication logic in Graphiti Core |\n",
    "\n",
    "**Scalability Implications:**\n",
    "\n",
    "- Python/LLM Layer: Processing involves heavy prompt construction, JSON parsing, and concurrent LLM requests. Graphiti Server is the layer that needs horizontal scaling (more Python workers).\n",
    "- DB Layer: Neo4j handles storage and retrieval efficiently. Unless the graph reaches hundreds of millions of nodes, the database is unlikely to be the primary bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.9 Comparison with VikingMem\n",
    "\n",
    "graphiti/zep is NOT a DBMS of agent memory, but VikingMem seems to be a DBMS. The core of zep/graphiti is still python client-side logic, still a hard-coded pipeline, using data shipping. \n",
    "- db usage:\n",
    "  - Yes, zep uses graph db to store all the data including raw data and higher-level 'views', but DB itself has no aware of what it stores.\n",
    "  - However, vikingmem, as I speculate based on the paper, likely has customized the computation/access layer of VikingDB, or perhaps it wraps VikingDB within a tightly integrated system shell.\n",
    "  - No built-in native operators such as SUM/AVG/LLM_MERGE in vikingmem (code/query shipping), no in-system processing, e.g., TTL, TIMECOMPRESS, LLM_MERGE are system primitives. Instead of DB->Python, it modifies the kernel/computation path.\n",
    "- Data model wise: \n",
    "  - Graphiti has schema (entity node, episode node, community node, etc.), but it seems to be descriptive, and the evolutionary logic between nodes is loose (flexible though, fully managed by LLM that may have hallunations);\n",
    "  - However, VikingMem's data model is presciptive and deterministic, as it has defined operators inside the data model. It explicitly states that the attribute Y of Entity X must and can only be calculated from the attribute B of Event A through the operator OP, attempting to make the evolution of memory a deterministic function.\n",
    "- Also, VikingMem has lifecycle management, making Consolidation (memory consolidation/forgetting) a bg automatic process (not explicitly called by the application layer).\n",
    "- Query optimization: graphiti's search is hard-coded python pipeline, but vikingmem mentioned multi-granular indexing and dynamic search, kinda meaning that system can automatically decide the best way to search, and use index effectively (query optimizer?).\n",
    "\n",
    "---\n",
    "\n",
    "### 10.10 Comparison with LOTUS\n",
    "\n",
    "Target:\n",
    "- LOTUS's target is to save money, sacraficing a little bit accuracy compared to full LLM queries；\n",
    "- Graphiti's main target is accuracy, not 'saving money'.\n",
    "\n",
    "Methodology:\n",
    "- Graphiti has 2 phases:\n",
    "  - create/update: expensive ETL pipeline, LLM heavy, creating and maintaining the view on top of raw data\n",
    "  - retrieval: only graph query, no LLM, fast, just retrieve the view\n",
    "- LOTUS has 1 phase:\n",
    "  - retrieval: no insertion and ETL, every operation is based on retrieval, LLM heavy, retrieve raw data, then process\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphiti-neo4j-otel)",
   "language": "python",
   "name": "graphiti-neo4j-otel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
